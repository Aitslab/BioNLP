{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log for project in Computer Science\n",
    "\n",
    "By Anna and Eric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1, 4/4 - 2019\n",
    "\n",
    "Today the goal is to understand the project and be prepared for mondays meeting.\n",
    "\n",
    "Step \n",
    "\n",
    "     1) Download UniProt and check out the dataset\n",
    "\n",
    "     2) Create the dictionary\n",
    "     \n",
    "     3) Get tool from Marcus\n",
    "     \n",
    "     4) Train a tagger with machine learning\n",
    "     \n",
    "So we would like to finish step 1 and start reseach on 2.\n",
    "\n",
    "---\n",
    "Today we sat from 13-16. We started by creating a private git repo with this file in it. Then we downloaded the uniprot dataset in xml form. We proceded to try and parse it with BeautifulSoup. The file was too big for that method.\n",
    "\n",
    "After that we tried with ElementTree instead which can go through each element one by one. This worked and we eventually got a list of proteins (over 200 000).\n",
    "\n",
    "We got stuck at which dictionary we want to create. Do we want one that's protein -> gene1 gene2 gene3 and thus also mine genes from the xml file. Or is the dictionary something else? For next time this needs to be cleared up in order to move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goes through the file uniprot_sprot.xml and saves the names of the proteins\n",
    "#in protein_names.txt\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='uniprot_sprot.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "for event, element in parser:\n",
    "    tag = element.tag[length:]\n",
    "    if 'shortName' in tag: # or 'shortName' in tag #fullName\n",
    "        proteins.add(element.text)\n",
    "    element.clear()\n",
    "f = open('protein_names_short.txt', 'w')\n",
    "f.write('\\n'.join(proteins))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 2, 8/4 - 2019\n",
    "\n",
    "Today we had our second meeting with Pierre and Sonja. We got clarification about the biology bit of the project but also some tips going forward. Now we have a Slack and are to get an invite to the Github repo. Also, an invite to Endnote.\n",
    "\n",
    "TODO: Invite Sonja, Pierre to Overleaf document\n",
    "\n",
    "Tips:\n",
    "\n",
    "    1) Search PubMed for 'biomedical text-mining' and 'bio NLP' (Jensen)\n",
    "    \n",
    "    2) Get annotation from Hannes\n",
    "\n",
    "So this week we wish to get the infrastructure up and running.\n",
    "\n",
    "---\n",
    "We started by looking at Marcus email and getting his program to work. After a lot of problems with installation of the various packages and programs we finally got it to work, almost 2h later.\n",
    "\n",
    "Question: What is the file 'disease_index.fst' and how is it created? Should we create this?\n",
    "\n",
    "---\n",
    "We now have the basic infrastructure up and running. We have done some tests on running it with our dataset of proteins from UniProt. Also we started looking at BioCreative's datasets, but we'll leave that for next time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions to start JVM (need Marcus repo):\n",
    "\n",
    "    1) cd [katalog]/mention-index-py4j\n",
    "\n",
    "    2) mvn package\n",
    "\n",
    "    3) cd target\n",
    "\n",
    "    4) java -jar mentions-index-py4j-1.0-SNAPSHOT.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a conda package in the current Jupyter kernel\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install docria\n",
    "#!{sys.executable} -m pip install py4j\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.printout import options\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    # Here we do the py4j equivalent for new java.io.File(path)\n",
    "    return jvm.java.io.File(os.path.abspath(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "app.buildIndex(get_java_file(\"protein_name/protein_names_long.txt\"), get_java_file(\"protein_name/protein_names_long.fst\"))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name/protein_names_long.fst\"))\n",
    "\n",
    "doc = Document()\n",
    "doc.add_text(\"main\",\"Peroxydase reaction stains were negative, chloroacetate esterase were strongly positive.\")\n",
    "binary_doc = MsgpackCodec.encode(doc) \n",
    "search_binary_doc = app.search(indx, binary_doc)\n",
    "doc = MsgpackCodec.decode(search_binary_doc)\n",
    "\n",
    "tuls = []\n",
    "for term in doc[\"matches\"]:\n",
    "    tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    #print(str(term[\"text\"]), \"-\", \"%d:%d\" % (term[\"text\"].start, term[\"text\"].stop))\n",
    "print(tuls)\n",
    "dominant_right(tuls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 3, 9/4-2019\n",
    "We have had some questions about the purpose and think we have finally figured it out. To identify the entities 'protein' and 'diseases' like cell death. To start with we will identify proteins by checking a pubmed article and using our uniprot list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goes through the file pubmed19n0651.xml and saves the abstract text as one textfile\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='pubmed19n0651.xml'\n",
    "out_name = 'abstract_text19n0651.txt'\n",
    "parser = ET.iterparse(filename)\n",
    "f=open(out_name, \"a+\", encoding=\"utf-8\")\n",
    "for event, element in parser:\n",
    "    if element.tag == 'AbstractText':\n",
    "        if element.text:\n",
    "            f.write(element.text)\n",
    "    element.clear()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Prints all occurences of proteins in the abstract file.\n",
    "app.buildIndex(get_java_file(\"protein_names_long.txt\"), get_java_file(\"protein_names_long.fst\"))\n",
    "indx = app.loadIndex(get_java_file(\"protein_names_long.fst\"))\n",
    "data = ''\n",
    "with open('abstract_text19n0651.txt', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('.')\n",
    "for s in sp:\n",
    "    hits = indx.search(s)\n",
    "    for hit in hits:\n",
    "        print(hit.start, hit.end, \" \".join([term.raw for term in hit.terms]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints all occurences of cell deaths in the abstract file.\n",
    "app.buildIndex(get_java_file(\"cell_death_names.txt\"), get_java_file(\"cell_name_index.fst\"))\n",
    "indx = app.loadIndex(get_java_file(\"cell_name_index.fst\"))\n",
    "data = ''\n",
    "with open('abstract_text19n0651.txt', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('.')\n",
    "for s in sp:\n",
    "    hits = indx.search(s)\n",
    "    for hit in hits:\n",
    "        print(hit.start, hit.end, \" \".join([term.raw for term in hit.terms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we tried to use the model that https://aclweb.org/anthology/W16-5104 implemented at github: https://github.com/withtwist/medical-ner. After transpiling the code to Python 2, changing the encoding of the data etc. we still got compilation errors. It seemed too big of a job so we scraped it. Since we have been able to identify proteins from the abstract and where they are located, we are now interested in finding a way to create the tracer with machine learning. We found the article *Unsupervised biomedical named entity recognition: Experiments with clinical and biological texts. \n",
    "\n",
    "\n",
    "Note: We noticed that the indx.search is case sensitive, which needs to be fixed. (e.g. toLowerCase or similar). Check w Marcus!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion of todays work: We have fixed the dictionaries and spent some time making sure the xml reader for uniprot included the edgecases and included 'shortName'. We also started tinkering with neural networks by downloading existing repos but we could not make it work. We started reading a couple of papers to get inspiration for our tagger. \n",
    "\n",
    "For next time, we will look deeper into the neural networks and use the tips Marcus sent us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 4, 14/4-2019\n",
    "We prepared for tomorrows meeting by looking at Marcus tips that he sent us. Also we started looking at more journals and papers and found a really good one: [A Novel Approach for Protein-Named Entity Recognition and Protein-Protein Interaction Extraction](https://www.hindawi.com/journals/mpe/2015/942435/). They describe step-by-step a method with SVM to extract the entities. If they agree on tomorrows meeting we will follow this guide next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 5, 15/4-2019\n",
    "Today after the meeting we have a clearer understanding how to move forward. The process follows:\n",
    "    1. Add list of corpus with summarization of each content and size to github\n",
    "    2. [x] Fix java to ignore case\n",
    "    3. [x] Complete a working model with the dictionary model\n",
    "        i. [x] ~~Fix reader and use Element tree instead.~~ Also read in 'pmid' and change output to 'pmid \\t abstract_text \\n'.\n",
    "        ii. [x] Match output to corpus annotation\n",
    "    4. [ ] Create evaluator for dictionary model\n",
    "        i. Use [genetag](https://www.ncbi.nlm.nih.gov/pubmed/15960837) for evaluation or chemner? or both?\n",
    "        ii. Compare with e.g recall\n",
    "    5. Add more to dictionary and run evaluator to see if it improves. If ambigueties use dominant write.\n",
    "    \n",
    "After this is completed we will start on the machine learning model.\n",
    "    1. Use POS? Pre-processing?\n",
    "    2. Sentence -> Tokens -> Tagger -> Phrases -> ML (BioLSTM and Bioascembedding(spelling?))\n",
    "    3. Check out page [nlpprogress](http://nlpprogress.com/english/named_entity_recognition.html) for NER datasets\n",
    "   \n",
    "\n",
    "MÃ¶te med Sonja: https://biocreative.bioinformatics.udel.edu/resources/corpora/biocreative-iii-corpus/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = app.loadIndex(get_java_file(\"protein_names_long.fst\"))\n",
    "\n",
    "data = ''\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    #If it is an excerpt\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID'):\n",
    "                id = line.split(': ')[1]\n",
    "            elif line.startswith('TEXT'):\n",
    "                text = ''.join(line.split(': ')[1:])\n",
    "        dictionary[id] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use indx.search to find all occurrences of proteins in abstracts from genetag. \n",
    "indx = app.loadIndex(get_java_file(\"protein_name_index.fst\"))\n",
    "\n",
    "d = dictionary\n",
    "out = []\n",
    "for key in d:\n",
    "    value = d[key]\n",
    "    hits = indx.search(value)    \n",
    "    for hit in hits:\n",
    "        # term.raw for term in hit.terms prints lists of protein names. Each list is protein name\n",
    "        out.append('|'.join([key, str(hit.start) + ' ' + str(hit.end), \" \".join([term.raw for term in hit.terms])]))\n",
    "file = open('genetag.out', 'w+', encoding=\"utf-8\", errors='ignore')\n",
    "file.write('\\n'.join(out))\n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "Today we sat for quite some time with merge conflicts in jupyter notebooks as it would not compile after a while. Also we tried to implement 2 models for evaluation; genetag with perl and another one with python 2.5. The gentag could have worked, but we got such a low score that it probably checks line by line simmilarity or something like it. This means we only got 8 correct matches which seems very low. We will look further into this next time.\n",
    "\n",
    "We succesfully changed the tokenizers in java by removing 2 of them and adding one to ignore cases. We became aware of how many small words were in the 'shortName' tag from chemprot. So we got protein names like 'AM, OR, AND' etc. We thus created a textfile 'protein_names_long' which contains only the long names. This meant we got a more expected result, but we are also missing out on some of the real names. TODO: Think about how to include the short names - do we want to set a limit of min 2 characters?\n",
    "\n",
    "Finally we created an out file to match the evaluation file in genetag. It reads from the database and using our proteinnames builds the same style file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 6, 17/4-2019\n",
    "We started by creating another file 'protein_names_short', and discussed whether to use this in combination with long names or only long names. For now we will only use long.\n",
    "\n",
    "Today's goal is to create/find/adapt an evaluator that works on our data.\n",
    "\n",
    "We can run the perl evaluator from genetag and get the following result:\n",
    "```\n",
    "TP: 2\n",
    "FP: 18211\n",
    "FN: 406\n",
    "Precision: 0.000109811672980838\n",
    "Recall: 0.00490196078431373\n",
    "```\n",
    "Which is very bad. we think this is because the java tokenizer changes words like \"h-55\" to \"h - 55\". So it matches both 'h' and '55'. We need to change such that the hyphen does not get split in this way. There are probably more errors as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))\n",
    "\n",
    "p_type = 'long'\n",
    "\n",
    "#app.buildIndex(get_java_file(\"protein_name/protein_names_{}.txt\".format(p_type)), get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "#indx = app.loadIndex(get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "\n",
    "\n",
    "#READ INPUT\n",
    "data = ''\n",
    "proteins = set()\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    #If it is an excerpt\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID'):\n",
    "                id = line.split(': ')[1]\n",
    "            elif line.startswith('TEXT'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        dictionary[id] = text\n",
    "    elif lines[0] == 'ANNOTATION':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        if 'ALTGENE' in lines[1]:\n",
    "            continue\n",
    "        for line in lines[2:]:\n",
    "            if line.startswith('TEXT:'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        proteins.add(text)\n",
    "print(len(proteins))\n",
    "with open('protein_name/protein_names_{}.txt'.format(p_type), 'a', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(proteins))\n",
    "\n",
    "app.buildIndex(get_java_file(\"protein_name/protein_names_{}.txt\".format(p_type)), get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "d = dictionary\n",
    "out = []\n",
    "\n",
    "for key in d:\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", d[key])    \n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)  \n",
    "\n",
    "    tuls = []\n",
    "    for term in doc[\"matches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    print(tuls)    \n",
    "    matches = [(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "    \n",
    "    for m in matches:\n",
    "        #if re.search('(?!^\\d+$)^.+$', m[2]): #removes pure numbers\n",
    "            out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "file = open('genetag/genetag_{}.out'.format(p_type), 'w+', encoding=\"utf-8\", errors='ignore')\n",
    "file.write('\\n'.join(out))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We chatted with Marcus for quite a bit today. As we wanted to add dominant_right we first tried to do it in Java but after a lot of trial-and-error it did not work and we switched to the python version. We also had to switch the way wee send in data as we needed to use docria instead of python.\n",
    "\n",
    "Additionally we started getting back to the filters in java that we originally had. We also tried to work on the evaluator, but we are suspecting that something is weird in it as we get an unexpected nbr of matches (only ~27 TruePositive). \n",
    "\n",
    "For next time: add stopwords and fix min_length of matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run the perl evaluator\n",
    "- go to medtag/genetag\n",
    "- run `perl alt_eval.perl ../[PATH]/genetag_short.out Gold.format > answer.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Day 7 6/5-2019\n",
    "Today's meeting: For next week we want a functioning demo. Want something to show eventhough it is not that accurate. GUI not priority but Pierre would like to do everything with the push of a button.\n",
    "\n",
    "Keep 2 layers, one fuzzy and remove the matches from fuzzy if it overlaps with the exact layer.\n",
    "\n",
    "TODO: Fix the Gold.format file. Right now it does not have the correct offset and thus gives us the terrible score. Second, we need to play around with the filters and keep building on 2 different analyzers for the short and long words. Finally tweak so that we can present a solution next time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Fix gold format file\n",
    "with open('../../medtag/genetag/Gold.format', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "\n",
    "prot = {}\n",
    "fixed = {} \n",
    "for line in data.split('\\n'):\n",
    "    l = line.split('|')\n",
    "    if len(l) > 2:\n",
    "        id = l[0]\n",
    "        [i1, i2] = l[1].split()\n",
    "        p = l[2]\n",
    "        prot.setdefault(id, []).append(p)\n",
    "\n",
    "for key in dictionary:\n",
    "    if key in prot:\n",
    "        proteins = prot[key]\n",
    "        start_i = 0\n",
    "        previous = ''\n",
    "        for p in proteins:\n",
    "            if p == previous:\n",
    "                start_i += len(previous)\n",
    "            match_start = start_i + dictionary[key][start_i:].index(p)\n",
    "            match_end = match_start + len(p)\n",
    "            start_i = match_start\n",
    "            fixed.setdefault(key, []).append('{}|{} {}|{}'.format(key, match_start, match_end, p))\n",
    "            previous = p\n",
    "\n",
    "elems = []\n",
    "for key in sorted(fixed.keys()):\n",
    "    for e in fixed[key]:\n",
    "        elems.append(e)\n",
    "\n",
    "with open('Gold2.format', 'w+', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(elems))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new Gold.format we got:\n",
    "\n",
    "genetag_long: ```Precision: 0.517406105864265 Recall: 0.350740713169061```. Gives: F1 = 0.42\n",
    "\n",
    "genetag_short: ```Precision: 0.11398451655411 Recall: 0.309527359475175```\n",
    "\n",
    "We changed the filters for genetag short (only minimum length = 2 and split on whitespace) and recieved a better result:\n",
    "\n",
    "genetag_short: ```Precision: 0.522418879056047 Recall: 0.291714709273596```. Gives: F1 = 0.37\n",
    "\n",
    "Now we also need to find a way to combine genetag_short and long and score them together. One idea is to use dominant right for each abstract. Another is to simply smack them together. As the evaluator is not dependent on the order of the id's, we will try the simple solution of stacking them together first.\n",
    "\n",
    "genetag: ```Precision: 0.288077969174977 Recall: 0.488497227255257```\n",
    "\n",
    "When stacking them together we don't get TP(short)+TP(long), so there seems to be an overlap of ~4000 proteins. So we must use something like dominant right to fix this overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build together long and short\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 8 7/5-2019\n",
    "We have consulted Marcus on what todo going forward with adding genetag_short and genetag_long. He said we have 2 options:\n",
    "    1. Add them both to one layer and do Dominant Right\n",
    "    2. Prioritize one detection method over the other, i.e. find all the intersections, remove the overlaps and then join the layers.\n",
    "\n",
    "So for today, our todo is as follows:\n",
    "    1. Remove numbers from the genetag lists (pure numbers like 555)\n",
    "    2. Join genetag long and short using one of the methods above\n",
    "    3. Evaluate and find possible improvements\n",
    "    4. Add list of cell death synonyms\n",
    "    5. Fix demo that we can show\n",
    "    \n",
    "---\n",
    "\n",
    "We will first look at the filters in java to see if there is one ready to use. There wasn't one. We will then do some postprocessing and remove those matches. We did this ```if re.search('(?!^\\d+$)^.+$', m[2]): #removes pure numbers``` which will suffice for now.\n",
    "\n",
    "genetag_short: ```Precision: 0.523598384077249 Recall: 0.291769615110086``` Without the only number ones. So slightly improved (by ~3 decimals).\n",
    "\n",
    "genetag_long: ```Precision: 0.336532053845129 Recall: 0.415911711414923``` So this got worse recall? What happened? \n",
    "\n",
    "(genetag_long: ```Precision: 0.336532053845129 Recall: 0.415911711414923``` F1 = 0.37. When we swiched ICUFoldingFilter to toLowerCaseFilter. So we'll go with ICU instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove numbers from the genetag lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying different filters for genetag_long:\n",
    "\n",
    "| Activated filter | Precision | Recall | F1 |\n",
    "|----------------------------------------|-----------|--------|----|\n",
    "| Mention, ICUNorm, Diacratic, Snowball  | 0.623715592889822 | 0.456596936254324 | 0.527 |\n",
    "| Mention, ICUNorm, Diacratic, Snowball, ICUFolding | 0.350802099229538 | 0.517487508922198 | 0.418 |\n",
    "| Mention, ICUNorm, Diacratic, Snowball, StopFilter | 0.615680473372781 | 0.457036182946247 | 0.524 |\n",
    "| Mention, ICUNorm, Diacratic, Snowball, LengthFilter | 0.287200716043858 | 0.422829846812716 | 0.342 |\n",
    "| Mention, ICUNorm, Diacratic, Snowball, toLowerCase | 0.350802099229538 | 0.517487508922198 | 0.418 |\n",
    "| Mention, ICUNorm, Diacratic | 0.651159123166693 | 0.45341239773788 | 0.535 |\n",
    "| Mention, ICUNorm |  0.651159123166693 | 0.45341239773788 | 0.535 |\n",
    "| Mention |  0.651159123166693 | 0.45341239773788 | 0.535 |\n",
    "| Mention (without removing numbers) |  0.651107782070488 | 0.45341239773788 | 0.535 |\n",
    "\n",
    "So for genetag_long the best solution seemed to be to not really add anything.\n",
    "\n",
    "---\n",
    "\n",
    "Different filters for genetag_short:\n",
    "\n",
    "| Activated filter | Precision | Recall | F1 |\n",
    "|----------------------------------------|-----------|--------|----|\n",
    "| PatternTokenizerFactory | 0.61393618220769 | 0.298962279690331 | 0.402 |\n",
    "| Mention | 0.629489888318744 | 0.458024488003075 | 0.530 |\n",
    "| Mention, LengthFilter | 0.324438202247191 | 0.418547191566464 | 0.366 |\n",
    "| Mention (without removing numbers) | 0.629489888318744 | 0.458024488003075 | 0.530 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join genetag long and short using one of the methods above\n",
    "data_long = ''\n",
    "data_short = ''\n",
    "\n",
    "out = []\n",
    "\n",
    "with open('genetag/genetag_long.out', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    data_long = file.read().split('\\n')\n",
    "with open('genetag/genetag_short.out', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    data_short = file.read().split('\\n')\n",
    "\n",
    "def read(data, dic):\n",
    "    for line in data:\n",
    "        tul = line.split('|')\n",
    "        key = tul[0]\n",
    "        line_nbr = tul[1].split()\n",
    "        start = line_nbr[0]\n",
    "        end = line_nbr[1]\n",
    "        word = tul[2]\n",
    "        dic.setdefault(key, []).append((int(start), int(end), word))\n",
    "    return dic\n",
    "\n",
    "dic = read(data_short, read(data_long, {}))\n",
    "\n",
    "for key in dic:\n",
    "    tuls = dic[key]\n",
    "    matches = [(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "    matches = list(set(matches))\n",
    "    \n",
    "    for m in matches:\n",
    "        out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "with open('genetag/genetag.out', 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined genetag_long and genetag_short, i.e. genetag.out: ```0.592672838290329 Recall: 0.495634985999012```\n",
    "F1 = 0.540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype\n",
    "text = 'Peroxydase reaction stains were negative, chloroacetate esterase were strongly positive.'\n",
    "file_name = 'protein_name/protein_names_long'\n",
    "\n",
    "\n",
    "def get_proteins(s, f):\n",
    "    from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "    import os.path\n",
    "    from docria.printout import options\n",
    "    from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "    set_large_screen()\n",
    "    def connect_jvm(port):\n",
    "        from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "        gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "        gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "        app = gateway.entry_point\n",
    "        return gateway, gateway.jvm, app\n",
    "    gateway, jvm, app = connect_jvm(6006)\n",
    "    def get_java_file(path):\n",
    "        # Here we do the py4j equivalent for new java.io.File(path)\n",
    "        return jvm.java.io.File(os.path.abspath(path))\n",
    "    app.buildIndex(get_java_file(\"{}.txt\".format(f)), get_java_file(\"{}.fst\".format(f)))\n",
    "    indx = app.loadIndex(get_java_file(\"{}.fst\".format(f)))\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", s)\n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)\n",
    "    tuls = []\n",
    "    for term in doc[\"matches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "        #print(str(term[\"text\"]), \"-\", \"%d:%d\" % (term[\"text\"].start, term[\"text\"].stop))\n",
    "    print(tuls)\n",
    "    return dominant_right(tuls)\n",
    "\n",
    "matches = get_proteins(text, file_name)\n",
    "print('--------')\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do everything for both short and long names\n",
    "name = 'protein_name/protein_names_all';\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='uniprot_sprot.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "with open('{}.txt'.format(name), 'w') as f:\n",
    "    for event, element in parser:\n",
    "        tag = element.tag[length:]\n",
    "        if 'shortName' == tag or 'fullName' == tag:\n",
    "            proteins.add(element.text)  \n",
    "        element.clear()\n",
    "    f.write('\\n'.join(proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))\n",
    "############################################################################\n",
    "\n",
    "app.buildIndex(get_java_file('{}.txt'.format(name)), get_java_file('{}.fst'.format(name)))\n",
    "indx = app.loadIndex(get_java_file('{}.fst'.format(name)))\n",
    "\n",
    "\n",
    "#READ INPUT\n",
    "data = ''\n",
    "proteins = set()\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    #If it is an excerpt\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID'):\n",
    "                id = line.split(': ')[1]\n",
    "            elif line.startswith('TEXT'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        dictionary[id] = text\n",
    "    elif lines[0] == 'ANNOTATION':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('TEXT:'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        proteins.add(text)\n",
    "\n",
    "with open('{}.txt'.format(name), 'a', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(proteins))\n",
    "file.close()\n",
    "\n",
    "app.buildIndex(get_java_file('{}.txt'.format(name)), get_java_file('{}.fst'.format(name)))\n",
    "indx = app.loadIndex(get_java_file('{}.fst'.format(name)))\n",
    "############################################################################\n",
    "#import re\n",
    "d = dictionary\n",
    "out = []\n",
    "\n",
    "for key in d:\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", d[key])    \n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)  \n",
    "\n",
    "    tuls = []\n",
    "    for term in doc[\"matches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    matches = [(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "    \n",
    "    for m in matches:\n",
    "        #if re.search('(?!^\\d+$)^.+$', m[2]): #removes pure numbers\n",
    "            out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "with open('genetag/genetag_all.out'.format(p_type), 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method above did not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 9, 9/5-2019\n",
    "From last time we still had the following tasks in our todo list left:\n",
    "    3. Evaluate and find possible improvements\n",
    "    4. Add list of cell death synonyms\n",
    "    5. Fix demo that we can show (!)\n",
    "    \n",
    "However, last time we ran into a problem with the evaluator (again!). It seems that the Gold.format does not include all the bits that is included in the annotation bits. Some of those seem to be included in Correct.format. What is the difference in these two? Where is the bug?\n",
    "\n",
    "We started by adding all the code we need below to prepare for the prototyping. Also, we ran the tests again to make sure that this did not change the outcome we got previously.\n",
    "\n",
    "Only do the following snippets once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goes through the file uniprot_sprot.xml and saves the names of the proteins\n",
    "#in protein_names.txt\n",
    "name = 'short'\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='uniprot_sprot.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "for event, element in parser:\n",
    "    tag = element.tag[length:]\n",
    "    if 'shortName' in tag: # or 'shortName' in tag #fullName\n",
    "        proteins.add(element.text)\n",
    "    element.clear()\n",
    "with open('protein_name/protein_names_{}.txt'.format(name), 'w+') as f:\n",
    "    f.write('\\n'.join(proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goes through the file uniprot_sprot.xml and saves the names of the proteins\n",
    "#in protein_names.txt\n",
    "name = 'long_no_anno'\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='uniprot_sprot.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "for event, element in parser:\n",
    "    tag = element.tag[length:]\n",
    "    if 'fullName' == tag: # or 'shortName' in tag #fullName\n",
    "        proteins.add(element.text)\n",
    "    element.clear()\n",
    "    \n",
    "with open('protein_name/protein_names_{}.txt'.format(name), 'w+') as f:\n",
    "    f.write('\\n'.join(proteins))\n",
    "\n",
    "#READ INPUT\n",
    "data = ''\n",
    "proteins = set()\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    if lines[0] == 'ANNOTATION' and 'ALTGENE' not in s:\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('TEXT:'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        proteins.add(text)\n",
    "\n",
    "with open('protein_name/protein_names_{}.txt'.format(name), 'a', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(proteins))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ ABSTRACTS TO FILE\n",
    "import json\n",
    "\n",
    "data = ''\n",
    "\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID'):\n",
    "                id = line.split(': ')[1]\n",
    "            elif line.startswith('TEXT'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        dictionary[id] = text\n",
    "\n",
    "with open('genetag/abstracts_genetag.txt', 'w+',encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write(json.dumps(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is to be run everytime there is a change in the Java code (i.e changed filters) or you want to switch between long/short words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "p_type = 'uniprot'\n",
    "c_type = 'cell_death_names'\n",
    "\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))\n",
    "\n",
    "app.buildIndex(get_java_file(\"protein_name/protein_names_{}.txt\".format(p_type)), get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "\n",
    "app.buildIndex(get_java_file(\"{}.txt\".format(p_type)), get_java_file(\"{}.fst\".format(p_type)))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "\n",
    "d = {}\n",
    "\n",
    "with open('genetag/abstracts_genetag.txt', 'r',encoding=\"utf-8\", errors='ignore') as file:\n",
    "    d = json.loads(file.read())\n",
    "\n",
    "    \n",
    "# Write to genetag\n",
    "out = []\n",
    "\n",
    "for key in d:\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", d[key])    \n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)  \n",
    "\n",
    "    tuls = []\n",
    "    for term in doc[\"protmatches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    matches = tuls #[(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "    \n",
    "    for m in matches:\n",
    "        #if re.search('(?!^\\d+$)^.+$', m[2]): #removes pure numbers\n",
    "        out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "with open('genetag/genetag_{}.out'.format(p_type), 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write('\\n'.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join genetag long and short\n",
    "data_long = ''\n",
    "data_short = ''\n",
    "\n",
    "out = []\n",
    "\n",
    "with open('genetag/genetag_long.out', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    data_long = file.read().split('\\n')\n",
    "with open('genetag/genetag_short.out', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    data_short = file.read().split('\\n')\n",
    "\n",
    "def read(data, dic):\n",
    "    for line in data:\n",
    "        tul = line.split('|')\n",
    "        key = tul[0]\n",
    "        line_nbr = tul[1].split()\n",
    "        start = line_nbr[0]\n",
    "        end = line_nbr[1]\n",
    "        word = tul[2]\n",
    "        dic.setdefault(key, []).append((int(start), int(end), word))\n",
    "    return dic\n",
    "\n",
    "dic = read(data_short, read(data_long, {}))\n",
    "\n",
    "for key in dic:\n",
    "    tuls = dic[key]\n",
    "    matches = [(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "    matches = list(set(matches))\n",
    "    \n",
    "    for m in matches:\n",
    "        out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "with open('genetag/genetag.out', 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype code and to use to test for bugs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype\n",
    "text = \"The MB-CPK isoenzyme showed no percentage increase of total CPK higher than 5%, measured at 6, 12, and 24 h after the shock, independent of the number of attempts of cardioversion.\"\n",
    "file_name = 'protein_name/protein_names_long'\n",
    "\n",
    "\n",
    "def get_proteins(s, f):\n",
    "    from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "    import os.path\n",
    "    from docria.printout import options\n",
    "    from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "    set_large_screen()\n",
    "    def connect_jvm(port):\n",
    "        from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "        gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "        gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "        app = gateway.entry_point\n",
    "        return gateway, gateway.jvm, app\n",
    "    gateway, jvm, app = connect_jvm(6006)\n",
    "    def get_java_file(path):\n",
    "        # Here we do the py4j equivalent for new java.io.File(path)\n",
    "        return jvm.java.io.File(os.path.abspath(path))\n",
    "    app.buildIndex(get_java_file(\"{}.txt\".format(f)), get_java_file(\"{}.fst\".format(f)))\n",
    "    indx = app.loadIndex(get_java_file(\"{}.fst\".format(f)))\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", s)\n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)\n",
    "    tuls = []\n",
    "    for term in doc[\"matches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "        #print(str(term[\"text\"]), \"-\", \"%d:%d\" % (term[\"text\"].start, term[\"text\"].stop))\n",
    "    print(tuls)\n",
    "    return dominant_right(tuls)\n",
    "\n",
    "matches = get_proteins(text, file_name)\n",
    "print('--------')\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New score after all of this:\n",
    "* TP: 8462\n",
    "* FP: 5066\n",
    "* FN: 9751\n",
    "* Precision: 0.62551744529864 Recall: 0.464613188381925\n",
    "* F1:0.533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix correct format file\n",
    "with open('../../medtag/genetag/Correct.data', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "\n",
    "#READ INPUT\n",
    "d = ''\n",
    "proteins = set()\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    d = file.read()\n",
    "    \n",
    "sp = d.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID:'):\n",
    "                id = line.split(': ')[1]\n",
    "            elif line.startswith('TEXT:'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        dictionary[id] = text\n",
    "        \n",
    "prot = {}\n",
    "fixed = {} \n",
    "\n",
    "for line in data.split('\\n'):\n",
    "    l = line.split('|')\n",
    "    if len(l) > 2:\n",
    "        id = l[0]\n",
    "        [i1, i2] = l[1].split()\n",
    "        p = l[2]\n",
    "        prot.setdefault(id, []).append(p)\n",
    "\n",
    "for key in dictionary:\n",
    "    if key in prot:\n",
    "        proteins = prot[key]\n",
    "        start_i = 0\n",
    "        previous = ''\n",
    "        for p in proteins:\n",
    "            if p == previous:\n",
    "                start_i += 0\n",
    "            if p in dictionary[key][start_i:]:\n",
    "                match_start = start_i + dictionary[key][start_i:].index(p)\n",
    "                match_end = match_start + len(p)\n",
    "                start_i = match_start\n",
    "                fixed.setdefault(key, []).append('{}|{} {}|{}'.format(key, match_start, match_end, p))\n",
    "                previous = p\n",
    "            else:\n",
    "                print(previous)\n",
    "                print(p)\n",
    "                print(dictionary[key])\n",
    "                print(dictionary[key][start_i:])\n",
    "                print('------------------------')\n",
    "\n",
    "elems = []\n",
    "for key in sorted(fixed.keys()):\n",
    "    for e in fixed[key]:\n",
    "        elems.append(e)\n",
    "\n",
    "with open('Correct2.data', 'w+', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(elems))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found a bug! When adding things to our dictionary we also add the 'ALTGENE', which is not included in the Gold.format file (it is in the Correct.data file). Thus things like this gets us in trouble:\n",
    "\n",
    "* ALTGENE: vesicular stomatitis virus glycoprotein (VSVG)\n",
    "* GENE: vesicular stomatitis virus glycoprotein\n",
    "* GENE: VSVG\n",
    "\n",
    "Because of dominant right we pick the ALTGENE as the correct one which Gold.format gives us the wrong answer for. Fixing that gives:\n",
    "\n",
    "* TP: 11046\n",
    "* FP: 2015\n",
    "* FN: 7167\n",
    "* Precision: 0.845723910879718 Recall: 0.606489869873168\n",
    "* F1 = 0.706\n",
    "\n",
    "---\n",
    "\n",
    "We found another bug:\n",
    "\n",
    "In the evaluation file:\n",
    "- FN|P07691837A1359|36 51|K3 keratin gene\n",
    "- TP|P07691837A1359|NFkB|185 189|NFkB|185 189\n",
    "\n",
    "In protein_names_long:\n",
    "- K3 keratin gene\n",
    "\n",
    "In genetag_long (nothing in genetag_short):\n",
    "- P07691837A1359|185 189|NFkB\n",
    "\n",
    "TEXT: \n",
    "- This 300 bp 5'-upstream sequence of K3 keratin gene, which can function in vitro as a keratinocyte-specific promoter, contains two clusters of partially overlapping motifs, one with an NFkB consensus sequence and another with a GC box.\n",
    "\n",
    "---\n",
    "Another bug:\n",
    "If we have the text\n",
    "- 'The **MB-CPK** isoenzyme showed no percentage increase of total CPK higher than 5%, measured at 6, 12, and 24 h after the shock, independent of the number of attempts of cardioversion.' \n",
    "\n",
    "We find CPK, but not MB. However if they switch places, i.e **'CPK-MB'**, then we only find MB and not CPK.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 10, 13/5-2019\n",
    "\n",
    "Today we need to solve the bugs from yesterday and also deliver a demo for tomorrow's meeting. Marcus told us to run the text through the analyzer pipeline and show the output to see if all the tokens are there. The MentionTokenizer might have some bugs, so we might to to switch that aswell.\n",
    "\n",
    "This is the output from the terms in 'The MB-CPK isoenzyme':\n",
    "\n",
    "```\n",
    "  field    value                   \n",
    "\n",
    "  term     'MB'                    \n",
    "  text     span(main[4:6]) = 'MB'  \n",
    "  type     'WORD_ACRONYM'          \n",
    "\n",
    "           Node terms#2           \n",
    "\n",
    "  field    value                  \n",
    "\n",
    "  term     '-'                    \n",
    "  text     span(main[6:7]) = '-'  \n",
    "  type     'HYPHEN'               \n",
    "\n",
    "            Node terms#3             \n",
    "\n",
    "  field    value                     \n",
    "\n",
    "  term     'CPK'                     \n",
    "  text     span(main[7:10]) = 'CPK'  \n",
    "  type     'WORD_ACRONYM'            \n",
    "\n",
    "                Node terms#4                \n",
    "``` \n",
    "\n",
    "And still we only get the match: [(7, 10, 'CPK')]\n",
    "\n",
    "---\n",
    "\n",
    "We found the bug! It was because of the dominant right we implemented in Java. We forgot about that since we have one in the pyhon code aswell.        //matches.retainAll(DominantRight.resolve(matches, \"text\"));\n",
    "\n",
    "Now we have the following score for genetag_long:\n",
    "\n",
    "```\n",
    "TP: 17090\n",
    "FP: 4845\n",
    "FN: 1123\n",
    "Precision: 0.779120127649875 Recall: 0.93834074562126\n",
    "```\n",
    "\n",
    "But genetag_short only gives:\n",
    "```\n",
    "TP: 2066\n",
    "FP: 4610\n",
    "FN: 16147\n",
    "Precision: 0.309466746554823 Recall: 0.113435458189206\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Apperantly we also need to divide the words from the annotaded part in genetag into training and test-set.\n",
    "\n",
    "genetag_long without the words in the annotations:\n",
    "```\n",
    "TP: 753\n",
    "FP: 2010\n",
    "FN: 17460\n",
    "Precision: 0.272529858849077 Recall: 0.0413440948772855\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide Gold into test and train\n",
    "div = 0.8\n",
    "\n",
    "gold = ''\n",
    "with open('Gold2.format', 'r') as f:\n",
    "    gold = f.read()\n",
    "\n",
    "gold = gold.split('\\n')\n",
    "\n",
    "train = round(len(gold)*div)\n",
    "\n",
    "train_gold = gold[:train]\n",
    "test_gold = gold[train:]\n",
    "\n",
    "with open('Gold2_train.format', 'w+', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(train_gold))\n",
    "    \n",
    "id = set()\n",
    "for g in test_gold:\n",
    "    id.add(g.split('|')[0])\n",
    "\n",
    "with open('Gold2_test.format', 'w+', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(test_gold))\n",
    "    \n",
    "with open('Gold2_test_ids.out', 'w+', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prot_names based on test and train\n",
    "name = 'long_no_anno'\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='uniprot_sprot.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "for event, element in parser:\n",
    "    tag = element.tag[length:]\n",
    "    if 'fullName' == tag: # or 'shortName' in tag #fullName\n",
    "        proteins.add(element.text)\n",
    "    element.clear()\n",
    "    \n",
    "with open('protein_name/protein_names_{}.txt'.format(name), 'w+') as f:\n",
    "    f.write('\\n'.join(proteins))\n",
    "\n",
    "p_id_test = []\n",
    "with open('Gold2_test_ids.out', 'r') as f:\n",
    "    p_id_test = f.read().split('\\n')\n",
    "\n",
    "#READ INPUT\n",
    "data = ''\n",
    "proteins = set()\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    if lines[0] == 'ANNOTATION' and 'ALTGENE' not in s:\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID:') and line.split('ID: ')[1] in p_id_test:\n",
    "                break\n",
    "            if line.startswith('TEXT:'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        proteins.add(text)\n",
    "\n",
    "with open('protein_name/protein_names_{}.txt'.format(name), 'a', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build genetag with prot names train\n",
    "\n",
    "import json\n",
    "\n",
    "p_type = 'uniprot'\n",
    "\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))\n",
    "\n",
    "app.buildIndex(get_java_file(\"protein_name/protein_names_{}.txt\".format(p_type)), get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "\n",
    "d = {}\n",
    "with open('genetag/abstracts_genetag.txt', 'r',encoding=\"utf-8\", errors='ignore') as file:\n",
    "    d = json.loads(file.read())\n",
    "\n",
    "p_id_test = []\n",
    "with open('Gold2_test_ids.out', 'r') as f:\n",
    "    p_id_test = f.read().split('\\n')\n",
    "    \n",
    "# Write to genetag\n",
    "out = []\n",
    "\n",
    "for key in d:\n",
    "    if key in p_id_test:\n",
    "        doc = Document()\n",
    "        doc.add_text(\"main\", d[key])    \n",
    "        binary_doc = MsgpackCodec.encode(doc) \n",
    "        search_binary_doc = app.search(indx, binary_doc)\n",
    "        doc = MsgpackCodec.decode(search_binary_doc)  \n",
    "\n",
    "        tuls = []\n",
    "        for term in doc[\"matches\"]:\n",
    "            tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "        matches = [(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "\n",
    "        for m in matches:\n",
    "            #if re.search('(?!^\\d+$)^.+$', m[2]): #removes pure numbers\n",
    "            out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "with open('genetag/genetag_{}.out'.format(p_type), 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PÃ¥ testset fÃ¥r vi dÃ¥ (med long bara):\n",
    "```\n",
    "TP: 1293\n",
    "FP: 1079\n",
    "FN: 2350\n",
    "Precision: 0.545109612141653 Recall: 0.354927257754598\n",
    "```\n",
    "PÃ¥ train fÃ¥r vi:\n",
    "```\n",
    "TP: 13744\n",
    "FP: 4101\n",
    "FN: 826\n",
    "Precision: 0.770187727654805 Recall: 0.943308167467399\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype\n",
    "\n",
    "import re\n",
    "\n",
    "def get_cell(text, f):\n",
    "    s_out = text\n",
    "    synonyms = []\n",
    "    with open(f, 'r') as f:\n",
    "        synonyms = f.read().split('\\n')\n",
    "    \n",
    "    for s in synonyms:\n",
    "        if s.lower() in text.lower():\n",
    "            s_out = re.sub(s, \"\\033[45m{}\\033[m\".format(s), s_out, flags = re.IGNORECASE)            \n",
    "    return s_out\n",
    "\n",
    "def get_proteins(s, f):\n",
    "    from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "    import os.path\n",
    "    from docria.printout import options\n",
    "    from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "    set_large_screen()\n",
    "    def connect_jvm(port):\n",
    "        from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "        gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "        gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "        app = gateway.entry_point\n",
    "        return gateway, gateway.jvm, app\n",
    "    gateway, jvm, app = connect_jvm(6006)\n",
    "    def get_java_file(path):\n",
    "        # Here we do the py4j equivalent for new java.io.File(path)\n",
    "        return jvm.java.io.File(os.path.abspath(path))\n",
    "    app.buildIndex(get_java_file(\"{}.txt\".format(f)), get_java_file(\"{}.fst\".format(f)))\n",
    "    indx = app.loadIndex(get_java_file(\"{}.fst\".format(f)))\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", s)\n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)\n",
    "    tuls = []\n",
    "    for term in doc[\"matches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "        #print(str(term[\"text\"]), \"-\", \"%d:%d\" % (term[\"text\"].start, term[\"text\"].stop))\n",
    "    tul = [x[2] for x in tuls]\n",
    "\n",
    "    dr = dominant_right(tuls)\n",
    "    #dr = tul # without dominant_right --> primase\n",
    "    \n",
    "    i = 0; j=0; s_out = ''\n",
    "\n",
    "    for d in dr:\n",
    "        while j < len(tuls):\n",
    "            j+=1\n",
    "            if d == tuls[j-1][2]:\n",
    "                print('MATCH:')\n",
    "                print(d)\n",
    "                print('------------------')\n",
    "                i_s = tuls[j-1][0]\n",
    "                i_e = tuls[j-1][1]\n",
    "\n",
    "                s_out += s[i:i_s] + \"\\033[43m{}\\033[m\".format(s[i_s:i_e])\n",
    "                i = i_e\n",
    "                break\n",
    "\n",
    "    return s_out + s[i:]\n",
    "\n",
    "\n",
    "# '\\033[43m{}\\033[m.format'\n",
    "#text = \"The MB-CPK isoenzyme showed no percentage increase of total CPK higher than 5%, measured at 6, 12, and 24 h after the shock, independent of the number of attempts of cardioversion. K3 keratin gene\"\n",
    "text = \"Apoptosis. The constraints of primase recognition sequences, nucleotide substrate requirements, and the effects of additional proteins on oligoribonucleotide synthesis by the 63-kDa gene 4 protein have been examined using templates of defined sequence. cell death is caused by cells dying.\"\n",
    "\n",
    "file_name = 'protein_name/protein_names_long'\n",
    "\n",
    "matches = \"{} \\n\\n\\033[43m{}\\033[m\\n\\033[45m{}\\033[m\".format(get_cell(get_proteins(text, file_name), 'cell_death_names.txt'), 'PROTEIN', 'CELL DEATH')\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "So today we finished a prototype that can highlight words in the terminal (probably won't work if we write to file), fixed the bugs by removing dominant right in java, and finally, splitting the annotated words into test/train data to use for the genetag which gave a worse result than before (as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Day 11, 14/5-2019\n",
    "\n",
    "Meeting w. Sonja and Pierre:\n",
    "* Collect all the names (even the alternative ones, use gene name as key, in some of them there are multiple gene ones - but not in the text file??), USE THE UNIPROT IDENTIFIER INSTEAD AS THE KEY!\n",
    "* Use spart of speach tagger to find what might be protein names and to remove unnecessary bits.\n",
    "* Use spacey\n",
    "* Apply this to a part of PUBMED (100 000- 200 000 nbr of abstracts) + benchmark the time it takes\n",
    "\n",
    "* Pro vs euc, 'taxanomic lineage', if it is pro we do not use those entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read the new uniprot file and store them in protein names\n",
    "filename='uniprot_less.txt'\n",
    "data = []\n",
    "with open(filename, 'r') as f:\n",
    "    data = f.read().split('//')[:-1] #remove last elem (only whitespace)\n",
    "\n",
    "dic = {}\n",
    "prots = []\n",
    "ids = []\n",
    "\n",
    "for entry in data:\n",
    "    lines = entry.split('\\n')\n",
    "    if 'OC   Eukaryota' not in entry:\n",
    "        continue\n",
    "    id = lines[1].split()[1]\n",
    "    prot = []\n",
    "    \n",
    "    for line in lines[2:]:\n",
    "        if line.startswith('DE') or line.startswith('GN'):\n",
    "            p = re.search('(?<!C)=.+?( {|;)', line)\n",
    "            if p:\n",
    "                prot.append(p.group()[1:-1])\n",
    "                prots.append(p.group()[1:-1])\n",
    "                ids.append(id)\n",
    "                \n",
    "    dic[id] = prot  \n",
    "    \n",
    "with open('protein_name/protein_names_uniprot.txt', 'w+') as f:\n",
    "    f.write('\\n'.join(prots))\n",
    "    \n",
    "with open('protein_name/protein_names_uniprot_indx.txt', 'w+') as f:\n",
    "    f.write('\\n'.join(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 12, 16/5 - 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print to file using docria and fix output for Vilhelm and Olof\n",
    "\n",
    "import re\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.printout import options\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "###\n",
    "from docria.storage import DocumentIO\n",
    "\n",
    "def get_proteins(s, f):\n",
    "    set_large_screen()\n",
    "    def connect_jvm(port):\n",
    "        from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "        gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "        gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "        app = gateway.entry_point\n",
    "        return gateway, gateway.jvm, app\n",
    "    gateway, jvm, app = connect_jvm(6006)\n",
    "    def get_java_file(path):\n",
    "        return jvm.java.io.File(os.path.abspath(path))\n",
    "    app.buildIndex(get_java_file(\"{}.txt\".format(f)), get_java_file(\"{}.fst\".format(f)))\n",
    "    app.buildIndex(get_java_file(\"cell_death_names.txt\"), get_java_file(\"cell_death_names.fst\"))\n",
    "\n",
    "    \n",
    "    indx = app.loadIndex(get_java_file(\"{}.fst\".format(f)))\n",
    "    indx2 = app.loadIndex(get_java_file(\"cell_death_names.fst\"))\n",
    "    \n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", s)   \n",
    "    \n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, indx2, binary_doc)\n",
    "\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)  \n",
    "    \n",
    "    tuls = []\n",
    "    for term in doc[\"protmatches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    \n",
    "    tuls2 = []\n",
    "    for term in doc[\"lysomatches\"]:\n",
    "        tuls2.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    \n",
    "    dr = dominant_right(tuls)\n",
    "    \n",
    "    i = 0; j=0; s_out = ''\n",
    "\n",
    "    for d in dr:\n",
    "        while j < len(tuls):\n",
    "            j+=1\n",
    "            if d == tuls[j-1][2]:\n",
    "                print('MATCH, PROT:')\n",
    "                print(d)\n",
    "                print('------------------')\n",
    "                i_s = tuls[j-1][0]\n",
    "                i_e = tuls[j-1][1]\n",
    "\n",
    "                s_out += s[i:i_s] + \"\\033[43m{}\\033[m\".format(s[i_s:i_e])\n",
    "                i = i_e\n",
    "                break\n",
    "    s_out += s[i:]\n",
    "\n",
    "    for d in tuls2:\n",
    "        print('MATCH, LYSO: ')\n",
    "        print(d[2])\n",
    "        print('------------------')\n",
    "        s_out = re.sub(d[2], \"\\033[45m{}\\033[m\".format(d[2]), s_out, flags = re.IGNORECASE) \n",
    "\n",
    "    with DocumentIO.write('output-file.docria') as dw:\n",
    "        dw.write(doc)\n",
    "    #with DocumentIO.read('output-file.docria') as dr:\n",
    "    #    for doc in dr:\n",
    "    #        print(doc[\"protmatches\"])  \n",
    "    #        print(doc[\"lysomatches\"])  \n",
    "                \n",
    "    return s_out\n",
    "\n",
    "\n",
    "# '\\033[43m{}\\033[m.format'\n",
    "#text = \"The MB-CPK isoenzyme showed no percentage increase of total CPK higher than 5%, measured at 6, 12, and 24 h after the shock, independent of the number of attempts of cardioversion. K3 keratin gene\"\n",
    "text = \"apoptosis. The constraints of Cr10HGO, 110 kDa antigen, and the effects of additional proteins on oligoribonucleotide synthesis by the 63-kDa gene 4 protein have been examined using templates of defined sequence. cell death is caused by cells dying.\"\n",
    "\n",
    "file_name = 'protein_name/protein_names_uniprot'\n",
    "\n",
    "matches = \"{} \\n\\n\\033[43m{}\\033[m\\n\\033[45m{}\\033[m\".format(get_proteins(text, file_name), 'PROTEIN', 'CELL DEATH')\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "p_type = 'uniprot(2)'\n",
    "c_type = 'cell_death_names'\n",
    "\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "from docria.storage import DocumentIO\n",
    "\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))\n",
    "\n",
    "app.buildIndex(get_java_file(\"protein_name/protein_names_{}.txt\".format(p_type)), get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "\n",
    "app.buildIndex(get_java_file(\"{}.txt\".format(c_type)), get_java_file(\"{}.fst\".format(c_type)))\n",
    "indx2 = app.loadIndex(get_java_file(\"{}.fst\".format(c_type)))\n",
    "\n",
    "d = {}\n",
    "\n",
    "with open('genetag/abstracts_genetag.txt', 'r',encoding=\"utf-8\", errors='ignore') as file:\n",
    "    d = json.loads(file.read())\n",
    "\n",
    "    \n",
    "# Write to genetag\n",
    "out = []\n",
    "dr = []\n",
    "for key in d:\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", d[key])  \n",
    "    doc.props[\"id\"] = key \n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, indx2, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)  \n",
    "\n",
    "    dr.append(doc)\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "while i < len(dr):\n",
    "    with DocumentIO.write('pubmed/pubmed1905({}).docria'.format(str(j))) as dw:\n",
    "        while i < len(dr) and i < 50000:\n",
    "            dw.write(dr[i])\n",
    "            i += 1\n",
    "    j += 1\n",
    "\n",
    "    #for m in matches:\n",
    "        #if re.search('(?!^\\d+$)^.+$', m[2]): #removes pure numbers\n",
    "    #    out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "#with open('genetag/genetag_{}.out'.format(p_type), 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "#    file.write('\\n'.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn docra file into readable temp file\n",
    "with DocumentIO.read('docria/out_json_pubmed19n0001.xml.txt.docria') as dr:\n",
    "    with open('docria/temp.txt', 'w+', encoding=\"utf-8\", errors='ignore') as f:\n",
    "        for doc in dr:\n",
    "            f.write(str(doc.text[\"main\"]))\n",
    "            f.write(str(doc.props))\n",
    "            for d in doc[\"protmatches\"]:\n",
    "                f.write(str(d))  \n",
    "            for d in doc[\"lysomatches\"]:\n",
    "                f.write(str(d))\n",
    "            f.write(\"--------------------\\n\\n\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append genetag names to protein names\n",
    "name = 'uniprot'\n",
    "\n",
    "prot_names = ''\n",
    "with open('protein_name/protein_names_{}.txt'.format(name), 'r') as f:\n",
    "    prot_names = f.read()\n",
    "\n",
    "#READ INPUT\n",
    "data = ''\n",
    "proteins = set()\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "for s in data.split('\\n>>'):\n",
    "    lines = s.split('\\n')\n",
    "    if lines[0] == 'ANNOTATION' and 'ALTGENE' not in s:\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('TEXT:'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        proteins.add(text)\n",
    "\n",
    "with open('protein_name/protein_names_{}(2).txt'.format(name), 'w+', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write(prot_names)\n",
    "    file.write('\\n'.join(proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "import json\n",
    "\n",
    "filename='pubmed19n0651.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "\n",
    "id = ''\n",
    "abbe = {}\n",
    "for event, element in parser:\n",
    "    tag = element.tag\n",
    "    if tag == 'PMID':\n",
    "        id = element.text\n",
    "    elif tag == 'AbstractText':\n",
    "        abbe.setdefault(id, []).append(element.text)\n",
    "    element.clear()\n",
    "\n",
    "dictionary = {}\n",
    "\n",
    "for key in abbe:\n",
    "    to_add = abbe[key]\n",
    "    if abbe[key] is str and len(abbe[key]) > 1:\n",
    "        to_add = ' '.join(abbe[key]) \n",
    "    dictionary[key] = to_add\n",
    "\n",
    "with open('pubmed/abstracts19n0651.txt', 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    for key in abbe:\n",
    "        file.write(key)\n",
    "        file.write(str(abbe[key]))\n",
    "        \n",
    "with open('pubmed/abstracts19n0651_json.txt', 'w+',encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write(json.dumps(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 13, 17/5\n",
    "Todays goals:\n",
    "1. Benchmark time it takes for pubmed CHECK!\n",
    "2. Experiment with spacey +  Try a part of speech tagger to improve findings CHECK!\n",
    "3. Find a way to remove noise, how much should we remove? CHECK!\n",
    "4. A method that takes a protein name and outputs it's genetag id? Or add it to docria?\n",
    "5. Clean up code CHECK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process protein names and cell death names using docria and print docria to file\n",
    "\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "from docria.storage import DocumentIO\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "set_large_screen()\n",
    "\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "\n",
    "def get_java_file(jvm, path):\n",
    "        return jvm.java.io.File(os.path.abspath(path))\n",
    "\n",
    "def setup_java(p, c):\n",
    "    gateway, jvm, app = connect_jvm(6006)\n",
    "    app.buildIndex(get_java_file(jvm, \"{}.txt\".format(p)), get_java_file(jvm, \"{}.fst\".format(p)))\n",
    "    app.buildIndex(get_java_file(jvm, \"{}.txt\".format(c)), get_java_file(jvm, \"{}.fst\".format(c)))\n",
    "\n",
    "    indx = app.loadIndex(get_java_file(jvm, \"{}.fst\".format(p)))\n",
    "    indx2 = app.loadIndex(get_java_file(jvm, \"{}.fst\".format(c)))\n",
    "    \n",
    "    return (app, indx, indx2)\n",
    "\n",
    "# Create a document and tag it\n",
    "def create_doc(id, text, app, i, i2):\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", text)  \n",
    "    doc.props[\"id\"] = id \n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(i, i2, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)\n",
    "    return doc\n",
    "\n",
    "def filter_away_nodes(d, stop_w):\n",
    "    rm_nodes = []\n",
    "    for node in d['protmatches']:\n",
    "        #If node is a stopword or number or length less than 2\n",
    "        if len(str(node[\"text\"])) < 2 or re.match( \"^[0-9-]+$\", str(node[\"text\"])) or str(node[\"text\"]) in stop_w:\n",
    "            rm_nodes.append(node)\n",
    "    [n.detach() for n in rm_nodes]\n",
    "    return d\n",
    "    \n",
    "#Taggs the abstracts in abstracts_genetag\n",
    "def file_tagger(app, indx, indx2, stop_w):\n",
    "    # Add the abstracts[key = PMID] = Abstract Text\n",
    "    abstracts = {}\n",
    "    with open('genetag/abstracts_genetag.txt', 'r',encoding=\"utf-8\", errors='ignore') as file:\n",
    "        abstracts = json.loads(file.read())\n",
    "\n",
    "    # One doc for each abstract\n",
    "    out = []; dr = []\n",
    "    for key in abstracts:\n",
    "        dr.append(create_doc(key, abstracts[key], app, indx, indx2))\n",
    "    for d in dr:\n",
    "        d = filter_away_nodes(d, stop_w)\n",
    "    \n",
    "    # Print one file per 50 000 abstracts\n",
    "    i = 0; j = 0\n",
    "    while i < len(dr):\n",
    "        with DocumentIO.write('pubmed/pubmed1905({}).docria'.format(str(j))) as dw:\n",
    "            while i < len(dr) and i < 50000:\n",
    "                dw.write(dr[i])\n",
    "                i += 1\n",
    "        j += 1\n",
    "    return len(dr)\n",
    "\n",
    "\n",
    "# Tags a given string, s, and prints matches and returns new string, s_out\n",
    "def text_tagger(s, app, indx, indx2):\n",
    "   \n",
    "    doc = filter_away_nodes(create_doc('id', s, app, indx, indx2))\n",
    "\n",
    "    tuls = []\n",
    "    for term in doc[\"protmatches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    \n",
    "    tuls2 = []\n",
    "    for term in doc[\"lysomatches\"]:\n",
    "        tuls2.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "        \n",
    "    i = 0; s_out = ''\n",
    "\n",
    "    for d in tuls:\n",
    "        print('------------------\\nMATCH, PROT:\\n{}\\n------------------'.format(d[2]))\n",
    "        i_s = d[0]\n",
    "        i_e = d[1]\n",
    "        s_out += s[i:i_s] + \"\\033[43m{}\\033[m\".format(s[i_s:i_e])\n",
    "        i = i_e\n",
    "        \n",
    "    s_out += s[i:]\n",
    "\n",
    "    for d in tuls2:\n",
    "        print('------------------\\nMATCH, LYSO:\\n{}\\n------------------'.format(d[2]))\n",
    "        s_out = re.sub(d[2], \"\\033[45m{}\\033[m\".format(d[2]), s_out, flags = re.IGNORECASE) \n",
    "    return s_out\n",
    "\n",
    "#A tagger which prints to genetag in eval format\n",
    "def score_tagger(c):\n",
    "    p = 'protein_name/protein_names_uniprot(2)'\n",
    "    \n",
    "    (app, indx, indx2) = setup_java(p, c)\n",
    "    \n",
    "    # Read dictionary with genetag abstracts \n",
    "    with open('genetag/abstracts_genetag.txt', 'r',encoding=\"utf-8\", errors='ignore') as file:\n",
    "        d = json.loads(file.read())\n",
    "\n",
    "    # Write to genetag\n",
    "    out = []\n",
    "\n",
    "    for key in d:\n",
    "        doc = filter_away_nodes(create_doc(key, d[key], app, indx, indx2))\n",
    "        tuls = []\n",
    "        \n",
    "        for term in doc[\"protmatches\"]:\n",
    "            tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "\n",
    "        for m in tuls:\n",
    "            out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "\n",
    "    with open('genetag/genetag_all.out', 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "        file.write('\\n'.join(out))\n",
    "        \n",
    "    return len(d)\n",
    "\n",
    "p = 'protein_name/protein_names_uniprot(2)'\n",
    "c = 'cell_death_names'\n",
    "\n",
    "stop_w = set()\n",
    "[stop_w.add(t) for t in stopwords.words('english')]\n",
    "\n",
    "(app, indx, indx2) = setup_java(p, c)\n",
    "\n",
    "#file_tagger = 'file', text_tagger = 'text',  score_tagger = 'score'\n",
    "run = 'file'\n",
    "\n",
    "if run == 'file':\n",
    "    start = time.time()\n",
    "    l = file_tagger(app, indx, indx2, stop_w)\n",
    "    end = time.time()\n",
    "    print('total time: {} s\\nnbr of abstracts: {}\\ntime per abstract: {} s'.format(str(end-start), l, str((end-start)/l)))\n",
    "\n",
    "elif run == 'text':\n",
    "    #text = 'Improvement of nursing instruction to be given at the time of discharge from the ward for premature infants'\n",
    "    text = \"the apoptosis the. The constraints of Cr10HGO, 110 kDa antigen, and the effects of additional proteins on oligoribonucleotide synthesis by the 63-kDa gene 4 protein have been examined using templates of defined sequence. cell death is caused by cells dying.\"\n",
    "    s = text_tagger(text, app, indx, indx2)\n",
    "    matches = \"{} \\n\\n\\033[43m{}\\033[m\\n\\033[45m{}\\033[m\".format(s, 'PROTEIN', 'CELL DEATH')\n",
    "    print(matches)\n",
    "\n",
    "elif run == 'score':\n",
    "    start = time.time()\n",
    "    l = score_tagger(c)\n",
    "    end = time.time()\n",
    "    print('total time: {} s\\ntime per abstract: {} s'.format(str(end-start), str((end-start)/l)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meeting with Marcus:\n",
    "* Our stopword filter is making things crazy, should be done in post instead\n",
    "* Use group by span in one of the layers in java\n",
    "* Use remove to delete layers\n",
    "\n",
    "* Without stopwordfilterfactory:\n",
    "```\n",
    "Accepting arbitrarily chosen limits of maximized errors of +/- 10%, it could be shown that the system did not work acceptably when the mean carbon dioxide concentration was below 1.5 vol.% within the fresh gas flow rates (2.2--7.7 1 min-1) and the range of minute ventilation (4--10 1 min-1) employed.{'id': 'P00155973A0387'}--------------------\n",
    "```\n",
    "* With stopfilterfactory\n",
    "\n",
    "```\n",
    "Accepting arbitrarily chosen limits of maximized errors of +/- 10%, it could be shown that the system did not work acceptably when the mean carbon dioxide concentration was below 1.5 vol.% within the fresh gas flow rates (2.2--7.7 1 min-1) and the range of minute ventilation (4--10 1 min-1) employed.{'id': 'P00155973A0387'}          Node protmatches#0          \n",
    "\n",
    "  field    value                      \n",
    "\n",
    "  id       42401                      \n",
    "  terms    [Node<terms#53>]           \n",
    "  text     span(main[231:232]) = '1'  \n",
    "          Node protmatches#1           \n",
    "\n",
    "  field    value                       \n",
    "\n",
    "  id       600341                      \n",
    "  terms    [Node<terms#53>]            \n",
    "  text     span(main[236:238]) = '-1'  \n",
    "          Node protmatches#2          \n",
    "\n",
    "  field    value                      \n",
    "\n",
    "  id       42401                      \n",
    "  terms    [Node<terms#53>]           \n",
    "  text     span(main[283:284]) = '1'  \n",
    "          Node protmatches#3           \n",
    "\n",
    "  field    value                       \n",
    "\n",
    "  id       600341                      \n",
    "  terms    [Node<terms#53>]            \n",
    "  text     span(main[288:290]) = '-1'  \n",
    "-------------------- \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 14, 21/5\n",
    "* Run through all pubmed abstracts and run the code on in\n",
    "* Write function to connect protein name to id -> maybe fix write to docria\n",
    "* Fix so that you can run through terminal and send files as input\n",
    "* Start looking at report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From xml.gz files to .xml \n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "path = 'pubmed2018/pubmed/'\n",
    "\n",
    "directory = os.fsencode(path)\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith('.xml.gz'):\n",
    "        with gzip.open(path + filename, 'rb') as f_in:\n",
    "            with open(path + filename[:-3], 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read xml files to json dictionaries\n",
    "from xml.etree import ElementTree as ET\n",
    "import json\n",
    "import os\n",
    "\n",
    "path = 'pubmed2018/pubmed/'\n",
    "\n",
    "directory = os.fsencode(path)\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    \n",
    "    if filename.endswith('.xml'):\n",
    "        parser = ET.iterparse(path + filename)\n",
    "        id = ''\n",
    "        abbe = {}\n",
    "        for event, element in parser:\n",
    "            tag = element.tag\n",
    "            if tag == 'PMID':\n",
    "                id = element.text\n",
    "            elif tag == 'AbstractText':\n",
    "                abbe.setdefault(id, []).append(element.text)\n",
    "            element.clear()\n",
    "\n",
    "        with open('pubmed/json_{}.txt'.format(filename[:-4]), 'w+',encoding=\"utf-8\", errors='ignore') as file:\n",
    "            file.write(json.dumps(abbe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2min 10 sec for 10 files => 12 s per each pubmed.xml\n",
    "We have 972 in total => 3,4 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dictionary with genetag abstracts \n",
    "with open('pubmed/json_pubmed19n0111.xml.txt', 'r',encoding=\"utf-8\", errors='ignore') as file:\n",
    "    d = json.loads(file.read())\n",
    "i = 0\n",
    "for key in d:\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(key)\n",
    "    print(d[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process protein names and cell death names using docria and print docria to file\n",
    "\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "from docria.storage import DocumentIO\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "\n",
    "set_large_screen()\n",
    "\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "\n",
    "def get_java_file(jvm, path):\n",
    "        return jvm.java.io.File(os.path.abspath(path))\n",
    "\n",
    "def setup_java(p, c):\n",
    "    gateway, jvm, app = connect_jvm(6006)\n",
    "    app.buildIndex(get_java_file(jvm, \"{}.txt\".format(p)), get_java_file(jvm, \"{}.fst\".format(p)))\n",
    "    app.buildIndex(get_java_file(jvm, \"{}.txt\".format(c)), get_java_file(jvm, \"{}.fst\".format(c)))\n",
    "\n",
    "    indx = app.loadIndex(get_java_file(jvm, \"{}.fst\".format(p)))\n",
    "    indx2 = app.loadIndex(get_java_file(jvm, \"{}.fst\".format(c)))\n",
    "    \n",
    "    return (app, indx, indx2)\n",
    "\n",
    "# Create a document and tag it\n",
    "def create_doc(id, text, app, i, i2):\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", text)  \n",
    "    doc.props[\"id\"] = id \n",
    "    binary_doc = MsgpackCodec.encode(doc)\n",
    "    search_binary_doc = app.search(i, i2, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)\n",
    "    return doc\n",
    "\n",
    "def filter_away_nodes(d, stop_w):\n",
    "    rm_nodes = []\n",
    "    for node in d['protmatches']:\n",
    "        #If node is a stopword or number or length less than 2\n",
    "        if len(str(node[\"text\"])) < 2 or re.match( \"^[0-9-]+$\", str(node[\"text\"])) or str(node[\"text\"]) in stop_w:\n",
    "            rm_nodes.append(node)\n",
    "    [n.detach() for n in rm_nodes]\n",
    "    return d\n",
    "    \n",
    "#Taggs the abstracts in abstracts_genetag\n",
    "def file_tagger(app, indx, indx2, stop_w, abstracts, filename):\n",
    "    # One doc for each abstract\n",
    "    out = []; dr = []\n",
    "    for key in abstracts:\n",
    "        text = ''.join(abstracts[key])\n",
    "        dr.append(create_doc(key, text, app, indx, indx2))\n",
    "    with DocumentIO.write('docria/out_{}.docria'.format(filename)) as dw:\n",
    "        for d in dr:\n",
    "            d = filter_away_nodes(d, stop_w)\n",
    "            dw.write(d)\n",
    "            \n",
    "    return len(dr)\n",
    "\n",
    "p = 'protein_name/protein_names_uniprot(2)'\n",
    "c = 'cell_death_names'\n",
    "path = 'pubmed/'\n",
    "\n",
    "\n",
    "stop_w = set()\n",
    "[stop_w.add(t) for t in stopwords.words('english')]\n",
    "\n",
    "(app, indx, indx2) = setup_java(p, c)\n",
    "\n",
    "directory = os.fsencode(path)\n",
    "\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if '19n0001' in filename or '19n0002' in filename:\n",
    "        continue\n",
    "    if filename.endswith('.xml.txt'):\n",
    "        with open(path + filename, 'r',encoding=\"utf-8\", errors='ignore') as file:\n",
    "            start = time.time()\n",
    "            d = json.loads(file.read())\n",
    "            l = file_tagger(app, indx, indx2, stop_w, d, filename)\n",
    "            end = time.time()\n",
    "            print('total time: {} s\\nnbr of abstracts: {}\\ntime per abstract: {} s'.format(str(end-start), l, str((end-start)/l)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```total time: 171.7357325553894 s\n",
    "nbr of abstracts: 15401\n",
    "time per abstract: 0.011150946857696863 s ```\n",
    "\n",
    "```total time: 144.01137924194336 s\n",
    "nbr of abstracts: 13455\n",
    "time per abstract: 0.010703186863020688 s ```\n",
    "\n",
    "```total time: 129.37624216079712 s\n",
    "nbr of abstracts: 12659\n",
    "time per abstract: 0.01022009970462099 s```\n",
    "\n",
    "```total time: 163.13451719284058 s\n",
    "nbr of abstracts: 14984\n",
    "time per abstract: 0.010887247543569178 s```\n",
    "\n",
    "```total time: 158.24822568893433 s\n",
    "nbr of abstracts: 14135\n",
    "time per abstract: 0.011195488198721919 s```\n",
    "\n",
    "```total time: 192.68476223945618 s\n",
    "nbr of abstracts: 16385\n",
    "time per abstract: 0.011759826807412645 s```\n",
    "\n",
    "```total time: 191.50953817367554 s\n",
    "nbr of abstracts: 16407\n",
    "time per abstract: 0.011672428730034469 s```\n",
    "\n",
    "```total time: 190.79297733306885 s\n",
    "nbr of abstracts: 15397\n",
    "time per abstract: 0.01239156831415658 s```\n",
    "\n",
    "```total time: 66.98976349830627 s\n",
    "nbr of abstracts: 5575\n",
    "time per abstract: 0.012016101075929377 s```\n",
    "\n",
    "```total time: 102.18244314193726 s\n",
    "nbr of abstracts: 9382\n",
    "time per abstract: 0.010891328409927228 s```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary about work since last meeting:\n",
    "We have\n",
    "* Collected all names from uniprot, only euc\n",
    "\n",
    "* Changed the filters in the java files and instead did it all in post processing as using the java files led to unexpected indexing. poS tagger/spacey -> solution takes too long/complicated, solved by checking if stopwords and lengthfilter which seemd to work great (see output)\n",
    "\n",
    "* Started benchmarking time -> takes ~14s to read a pubmed.xml file + ~40s to process with docria = ~1 min\n",
    "\n",
    "* Docria output has one layer for lyso matches and one for prot matches. Should we append genetag names to this?\n",
    "\n",
    "* Wrote a method that takes a protein name and outputs the genetag id (perhaps add to docria)\n",
    "\n",
    "* Started to process all the abstracts\n",
    "\n",
    "* Time to run everything; almost 2 days"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
