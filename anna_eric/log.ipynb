{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log for project in Computer Science\n",
    "\n",
    "By Anna and Eric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1, 4/4 - 2019\n",
    "\n",
    "Today the goal is to understand the project and be prepared for mondays meeting.\n",
    "\n",
    "Step \n",
    "\n",
    "     1) Download UniProt and check out the dataset\n",
    "\n",
    "     2) Create the dictionary\n",
    "     \n",
    "     3) Get tool from Marcus\n",
    "     \n",
    "     4) Train a tagger with machine learning\n",
    "     \n",
    "So we would like to finish step 1 and start reseach on 2.\n",
    "\n",
    "---\n",
    "Today we sat from 13-16. We started by creating a private git repo with this file in it. Then we downloaded the uniprot dataset in xml form. We proceded to try and parse it with BeautifulSoup. The file was too big for that method.\n",
    "\n",
    "After that we tried with ElementTree instead which can go through each element one by one. This worked and we eventually got a list of proteins (over 200 000).\n",
    "\n",
    "We got stuck at which dictionary we want to create. Do we want one that's protein -> gene1 gene2 gene3 and thus also mine genes from the xml file. Or is the dictionary something else? For next time this needs to be cleared up in order to move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goes through the file uniprot_sprot.xml and saves the names of the proteins\n",
    "#in protein_names.txt\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='uniprot_sprot.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "for event, element in parser:\n",
    "    tag = element.tag[length:]\n",
    "    if 'shortName' in tag: # or 'shortName' in tag #fullName\n",
    "        proteins.add(element.text)\n",
    "    element.clear()\n",
    "f = open('protein_names_short.txt', 'w')\n",
    "f.write('\\n'.join(proteins))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 2, 8/4 - 2019\n",
    "\n",
    "Today we had our second meeting with Pierre and Sonja. We got clarification about the biology bit of the project but also some tips going forward. Now we have a Slack and are to get an invite to the Github repo. Also, an invite to Endnote.\n",
    "\n",
    "TODO: Invite Sonja, Pierre to Overleaf document\n",
    "\n",
    "Tips:\n",
    "\n",
    "    1) Search PubMed for 'biomedical text-mining' and 'bio NLP' (Jensen)\n",
    "    \n",
    "    2) Get annotation from Hannes\n",
    "\n",
    "So this week we wish to get the infrastructure up and running.\n",
    "\n",
    "---\n",
    "We started by looking at Marcus email and getting his program to work. After a lot of problems with installation of the various packages and programs we finally got it to work, almost 2h later.\n",
    "\n",
    "Question: What is the file 'disease_index.fst' and how is it created? Should we create this?\n",
    "\n",
    "---\n",
    "We now have the basic infrastructure up and running. We have done some tests on running it with our dataset of proteins from UniProt. Also we started looking at BioCreative's datasets, but we'll leave that for next time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions to start JVM (need Marcus repo):\n",
    "\n",
    "    1) cd [katalog]/mention-index-py4j\n",
    "\n",
    "    2) mvn package\n",
    "\n",
    "    3) cd target\n",
    "\n",
    "    4) java -jar mentions-index-py4j-1.0-SNAPSHOT.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a conda package in the current Jupyter kernel\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install docria\n",
    "#!{sys.executable} -m pip install py4j\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.printout import options\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    # Here we do the py4j equivalent for new java.io.File(path)\n",
    "    return jvm.java.io.File(os.path.abspath(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "app.buildIndex(get_java_file(\"protein_name/protein_names_long.txt\"), get_java_file(\"protein_name/protein_names_long.fst\"))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name/protein_names_long.fst\"))\n",
    "\n",
    "doc = Document()\n",
    "doc.add_text(\"main\",\"Peroxydase reaction stains were negative, chloroacetate esterase were strongly positive.\")\n",
    "binary_doc = MsgpackCodec.encode(doc) \n",
    "search_binary_doc = app.search(indx, binary_doc)\n",
    "doc = MsgpackCodec.decode(search_binary_doc)\n",
    "\n",
    "tuls = []\n",
    "for term in doc[\"matches\"]:\n",
    "    tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    #print(str(term[\"text\"]), \"-\", \"%d:%d\" % (term[\"text\"].start, term[\"text\"].stop))\n",
    "print(tuls)\n",
    "dominant_right(tuls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 3, 9/4-2019\n",
    "We have had some questions about the purpose and think we have finally figured it out. To identify the entities 'protein' and 'diseases' like cell death. To start with we will identify proteins by checking a pubmed article and using our uniprot list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goes through the file pubmed19n0651.xml and saves the abstract text as one textfile\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='pubmed19n0651.xml'\n",
    "out_name = 'abstract_text19n0651.txt'\n",
    "parser = ET.iterparse(filename)\n",
    "f=open(out_name, \"a+\", encoding=\"utf-8\")\n",
    "for event, element in parser:\n",
    "    if element.tag == 'AbstractText':\n",
    "        if element.text:\n",
    "            f.write(element.text)\n",
    "    element.clear()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Prints all occurences of proteins in the abstract file.\n",
    "app.buildIndex(get_java_file(\"protein_names_long.txt\"), get_java_file(\"protein_names_long.fst\"))\n",
    "indx = app.loadIndex(get_java_file(\"protein_names_long.fst\"))\n",
    "data = ''\n",
    "with open('abstract_text19n0651.txt', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('.')\n",
    "for s in sp:\n",
    "    hits = indx.search(s)\n",
    "    for hit in hits:\n",
    "        print(hit.start, hit.end, \" \".join([term.raw for term in hit.terms]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints all occurences of cell deaths in the abstract file.\n",
    "app.buildIndex(get_java_file(\"cell_death_names.txt\"), get_java_file(\"cell_name_index.fst\"))\n",
    "indx = app.loadIndex(get_java_file(\"cell_name_index.fst\"))\n",
    "data = ''\n",
    "with open('abstract_text19n0651.txt', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('.')\n",
    "for s in sp:\n",
    "    hits = indx.search(s)\n",
    "    for hit in hits:\n",
    "        print(hit.start, hit.end, \" \".join([term.raw for term in hit.terms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we tried to use the model that https://aclweb.org/anthology/W16-5104 implemented at github: https://github.com/withtwist/medical-ner. After transpiling the code to Python 2, changing the encoding of the data etc. we still got compilation errors. It seemed too big of a job so we scraped it. Since we have been able to identify proteins from the abstract and where they are located, we are now interested in finding a way to create the tracer with machine learning. We found the article *Unsupervised biomedical named entity recognition: Experiments with clinical and biological texts. \n",
    "\n",
    "\n",
    "Note: We noticed that the indx.search is case sensitive, which needs to be fixed. (e.g. toLowerCase or similar). Check w Marcus!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion of todays work: We have fixed the dictionaries and spent some time making sure the xml reader for uniprot included the edgecases and included 'shortName'. We also started tinkering with neural networks by downloading existing repos but we could not make it work. We started reading a couple of papers to get inspiration for our tagger. \n",
    "\n",
    "For next time, we will look deeper into the neural networks and use the tips Marcus sent us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 4, 14/4-2019\n",
    "We prepared for tomorrows meeting by looking at Marcus tips that he sent us. Also we started looking at more journals and papers and found a really good one: [A Novel Approach for Protein-Named Entity Recognition and Protein-Protein Interaction Extraction](https://www.hindawi.com/journals/mpe/2015/942435/). They describe step-by-step a method with SVM to extract the entities. If they agree on tomorrows meeting we will follow this guide next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 5, 15/4-2019\n",
    "Today after the meeting we have a clearer understanding how to move forward. The process follows:\n",
    "    1. Add list of corpus with summarization of each content and size to github\n",
    "    2. [x] Fix java to ignore case\n",
    "    3. [x] Complete a working model with the dictionary model\n",
    "        i. [x] ~~Fix reader and use Element tree instead.~~ Also read in 'pmid' and change output to 'pmid \\t abstract_text \\n'.\n",
    "        ii. [x] Match output to corpus annotation\n",
    "    4. [ ] Create evaluator for dictionary model\n",
    "        i. Use [genetag](https://www.ncbi.nlm.nih.gov/pubmed/15960837) for evaluation or chemner? or both?\n",
    "        ii. Compare with e.g recall\n",
    "    5. Add more to dictionary and run evaluator to see if it improves. If ambigueties use dominant write.\n",
    "    \n",
    "After this is completed we will start on the machine learning model.\n",
    "    1. Use POS? Pre-processing?\n",
    "    2. Sentence -> Tokens -> Tagger -> Phrases -> ML (BioLSTM and Bioascembedding(spelling?))\n",
    "    3. Check out page [nlpprogress](http://nlpprogress.com/english/named_entity_recognition.html) for NER datasets\n",
    "   \n",
    "\n",
    "MÃ¶te med Sonja: https://biocreative.bioinformatics.udel.edu/resources/corpora/biocreative-iii-corpus/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = app.loadIndex(get_java_file(\"protein_names_long.fst\"))\n",
    "\n",
    "data = ''\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    #If it is an excerpt\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID'):\n",
    "                id = line.split(': ')[1]\n",
    "            elif line.startswith('TEXT'):\n",
    "                text = ''.join(line.split(': ')[1:])\n",
    "        dictionary[id] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use indx.search to find all occurrences of proteins in abstracts from genetag. \n",
    "indx = app.loadIndex(get_java_file(\"protein_name_index.fst\"))\n",
    "\n",
    "d = dictionary\n",
    "out = []\n",
    "for key in d:\n",
    "    value = d[key]\n",
    "    hits = indx.search(value)    \n",
    "    for hit in hits:\n",
    "        # term.raw for term in hit.terms prints lists of protein names. Each list is protein name\n",
    "        out.append('|'.join([key, str(hit.start) + ' ' + str(hit.end), \" \".join([term.raw for term in hit.terms])]))\n",
    "file = open('genetag.out', 'w+', encoding=\"utf-8\", errors='ignore')\n",
    "file.write('\\n'.join(out))\n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "Today we sat for quite some time with merge conflicts in jupyter notebooks as it would not compile after a while. Also we tried to implement 2 models for evaluation; genetag with perl and another one with python 2.5. The gentag could have worked, but we got such a low score that it probably checks line by line simmilarity or something like it. This means we only got 8 correct matches which seems very low. We will look further into this next time.\n",
    "\n",
    "We succesfully changed the tokenizers in java by removing 2 of them and adding one to ignore cases. We became aware of how many small words were in the 'shortName' tag from chemprot. So we got protein names like 'AM, OR, AND' etc. We thus created a textfile 'protein_names_long' which contains only the long names. This meant we got a more expected result, but we are also missing out on some of the real names. TODO: Think about how to include the short names - do we want to set a limit of min 2 characters?\n",
    "\n",
    "Finally we created an out file to match the evaluation file in genetag. It reads from the database and using our proteinnames builds the same style file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 6, 17/4-2019\n",
    "We started by creating another file 'protein_names_short', and discussed whether to use this in combination with long names or only long names. For now we will only use long.\n",
    "\n",
    "Today's goal is to create/find/adapt an evaluator that works on our data.\n",
    "\n",
    "We can run the perl evaluator from genetag and get the following result:\n",
    "```\n",
    "TP: 2\n",
    "FP: 18211\n",
    "FN: 406\n",
    "Precision: 0.000109811672980838\n",
    "Recall: 0.00490196078431373\n",
    "```\n",
    "Which is very bad. we think this is because the java tokenizer changes words like \"h-55\" to \"h - 55\". So it matches both 'h' and '55'. We need to change such that the hyphen does not get split in this way. There are probably more errors as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))\n",
    "\n",
    "p_type = 'long'\n",
    "\n",
    "#app.buildIndex(get_java_file(\"protein_name/protein_names_{}.txt\".format(p_type)), get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "#indx = app.loadIndex(get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "\n",
    "\n",
    "#READ INPUT\n",
    "data = ''\n",
    "proteins = set()\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    #If it is an excerpt\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID'):\n",
    "                id = line.split(': ')[1]\n",
    "            elif line.startswith('TEXT'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        dictionary[id] = text\n",
    "    elif lines[0] == 'ANNOTATION':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('TEXT:'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        proteins.add(text)\n",
    "print(len(proteins))\n",
    "with open('protein_name/protein_names_{}.txt'.format(p_type), 'a', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(proteins))\n",
    "\n",
    "app.buildIndex(get_java_file(\"protein_name/protein_names_{}.txt\".format(p_type)), get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name/protein_names_{}.fst\".format(p_type)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "d = dictionary\n",
    "out = []\n",
    "\n",
    "for key in d:\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", d[key])    \n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)  \n",
    "\n",
    "    tuls = []\n",
    "    for term in doc[\"matches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    print(tuls)    \n",
    "    matches = [(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "    \n",
    "    for m in matches:\n",
    "        #if re.search('(?!^\\d+$)^.+$', m[2]): #removes pure numbers\n",
    "            out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "file = open('genetag/genetag_{}.out'.format(p_type), 'w+', encoding=\"utf-8\", errors='ignore')\n",
    "file.write('\\n'.join(out))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We chatted with Marcus for quite a bit today. As we wanted to add dominant_right we first tried to do it in Java but after a lot of trial-and-error it did not work and we switched to the python version. We also had to switch the way wee send in data as we needed to use docria instead of python.\n",
    "\n",
    "Additionally we started getting back to the filters in java that we originally had. We also tried to work on the evaluator, but we are suspecting that something is weird in it as we get an unexpected nbr of matches (only ~27 TruePositive). \n",
    "\n",
    "For next time: add stopwords and fix min_length of matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run the perl evaluator\n",
    "- go to medtag/genetag\n",
    "- run `perl alt_eval.perl ../[PATH]/genetag_short.out Gold.format > answer.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Day 7 6/5-2019\n",
    "Today's meeting: For next week we want a functioning demo. Want something to show eventhough it is not that accurate. GUI not priority but Pierre would like to do everything with the push of a button.\n",
    "\n",
    "Keep 2 layers, one fuzzy and remove the matches from fuzzy if it overlaps with the exact layer.\n",
    "\n",
    "TODO: Fix the Gold.format file. Right now it does not have the correct offset and thus gives us the terrible score. Second, we need to play around with the filters and keep building on 2 different analyzers for the short and long words. Finally tweak so that we can present a solution next time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Fix gold format file\n",
    "with open('../../medtag/genetag/Gold.format', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "\n",
    "prot = {}\n",
    "fixed = {} \n",
    "for line in data.split('\\n'):\n",
    "    l = line.split('|')\n",
    "    if len(l) > 2:\n",
    "        id = l[0]\n",
    "        [i1, i2] = l[1].split()\n",
    "        p = l[2]\n",
    "        prot.setdefault(id, []).append(p)\n",
    "\n",
    "for key in dictionary:\n",
    "    if key in prot:\n",
    "        proteins = prot[key]\n",
    "        start_i = 0\n",
    "        previous = ''\n",
    "        for p in proteins:\n",
    "            if p == previous:\n",
    "                start_i += len(previous)\n",
    "            match_start = start_i + dictionary[key][start_i:].index(p)\n",
    "            match_end = match_start + len(p)\n",
    "            start_i = match_start\n",
    "            fixed.setdefault(key, []).append('{}|{} {}|{}'.format(key, match_start, match_end, p))\n",
    "            previous = p\n",
    "\n",
    "elems = []\n",
    "for key in sorted(fixed.keys()):\n",
    "    for e in fixed[key]:\n",
    "        elems.append(e)\n",
    "\n",
    "with open('Gold2.format', 'w', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(elems))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new Gold.format we got:\n",
    "\n",
    "genetag_long: ```Precision: 0.517406105864265 Recall: 0.350740713169061```. Gives: F1 = 0.42\n",
    "\n",
    "genetag_short: ```Precision: 0.11398451655411 Recall: 0.309527359475175```\n",
    "\n",
    "We changed the filters for genetag short (only minimum length = 2 and split on whitespace) and recieved a better result:\n",
    "\n",
    "genetag_short: ```Precision: 0.522418879056047 Recall: 0.291714709273596```. Gives: F1 = 0.37\n",
    "\n",
    "Now we also need to find a way to combine genetag_short and long and score them together. One idea is to use dominant right for each abstract. Another is to simply smack them together. As the evaluator is not dependent on the order of the id's, we will try the simple solution of stacking them together first.\n",
    "\n",
    "genetag: ```Precision: 0.288077969174977 Recall: 0.488497227255257```\n",
    "\n",
    "When stacking them together we don't get TP(short)+TP(long), so there seems to be an overlap of ~4000 proteins. So we must use something like dominant right to fix this overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build together long and short\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 8 7/5-2019\n",
    "We have consulted Marcus on what todo going forward with adding genetag_short and genetag_long. He said we have 2 options:\n",
    "    1. Add them both to one layer and do Dominant Right\n",
    "    2. Prioritize one detection method over the other, i.e. find all the intersections, remove the overlaps and then join the layers.\n",
    "\n",
    "So for today, our todo is as follows:\n",
    "    1. Remove numbers from the genetag lists (pure numbers like 555)\n",
    "    2. Join genetag long and short using one of the methods above\n",
    "    3. Evaluate and find possible improvements\n",
    "    4. Add list of cell death synonyms\n",
    "    5. Fix demo that we can show\n",
    "    \n",
    "---\n",
    "\n",
    "We will first look at the filters in java to see if there is one ready to use. There wasn't one. We will then do some postprocessing and remove those matches. We did this ```if re.search('(?!^\\d+$)^.+$', m[2]): #removes pure numbers``` which will suffice for now.\n",
    "\n",
    "genetag_short: ```Precision: 0.523598384077249 Recall: 0.291769615110086``` Without the only number ones. So slightly improved (by ~3 decimals).\n",
    "\n",
    "genetag_long: ```Precision: 0.336532053845129 Recall: 0.415911711414923``` So this got worse recall? What happened? \n",
    "\n",
    "(genetag_long: ```Precision: 0.336532053845129 Recall: 0.415911711414923``` F1 = 0.37. When we swiched ICUFoldingFilter to toLowerCaseFilter. So we'll go with ICU instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove numbers from the genetag lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying different filters for genetag_long:\n",
    "\n",
    "| Activated filter | Precision | Recall | F1 |\n",
    "|----------------------------------------|-----------|--------|----|\n",
    "| Mention, ICUNorm, Diacratic, Snowball  | 0.623715592889822 | 0.456596936254324 | 0.527 |\n",
    "| Mention, ICUNorm, Diacratic, Snowball, ICUFolding | 0.350802099229538 | 0.517487508922198 | 0.418 |\n",
    "| Mention, ICUNorm, Diacratic, Snowball, StopFilter | 0.615680473372781 | 0.457036182946247 | 0.524 |\n",
    "| Mention, ICUNorm, Diacratic, Snowball, LengthFilter | 0.287200716043858 | 0.422829846812716 | 0.342 |\n",
    "| Mention, ICUNorm, Diacratic, Snowball, toLowerCase | 0.350802099229538 | 0.517487508922198 | 0.418 |\n",
    "| Mention, ICUNorm, Diacratic | 0.651159123166693 | 0.45341239773788 | 0.535 |\n",
    "| Mention, ICUNorm |  0.651159123166693 | 0.45341239773788 | 0.535 |\n",
    "| Mention |  0.651159123166693 | 0.45341239773788 | 0.535 |\n",
    "| Mention (without removing numbers) |  0.651107782070488 | 0.45341239773788 | 0.535 |\n",
    "\n",
    "So for genetag_long the best solution seemed to be to not really add anything.\n",
    "\n",
    "---\n",
    "\n",
    "Different filters for genetag_short:\n",
    "\n",
    "| Activated filter | Precision | Recall | F1 |\n",
    "|----------------------------------------|-----------|--------|----|\n",
    "| PatternTokenizerFactory | 0.61393618220769 | 0.298962279690331 | 0.402 |\n",
    "| Mention | 0.629489888318744 | 0.458024488003075 | 0.530 |\n",
    "| Mention, LengthFilter | 0.324438202247191 | 0.418547191566464 | 0.366 |\n",
    "| Mention (without removing numbers) | 0.629489888318744 | 0.458024488003075 | 0.530 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join genetag long and short using one of the methods above\n",
    "data_long = ''\n",
    "data_short = ''\n",
    "\n",
    "out = []\n",
    "\n",
    "with open('genetag/genetag_long.out', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    data_long = file.read().split('\\n')\n",
    "with open('genetag/genetag_short.out', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    data_short = file.read().split('\\n')\n",
    "\n",
    "def read(data, dic):\n",
    "    for line in data:\n",
    "        tul = line.split('|')\n",
    "        key = tul[0]\n",
    "        line_nbr = tul[1].split()\n",
    "        start = line_nbr[0]\n",
    "        end = line_nbr[1]\n",
    "        word = tul[2]\n",
    "        dic.setdefault(key, []).append((int(start), int(end), word))\n",
    "    return dic\n",
    "\n",
    "dic = read(data_short, read(data_long, {}))\n",
    "\n",
    "for key in dic:\n",
    "    tuls = dic[key]\n",
    "    matches = [(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "    matches = list(set(matches))\n",
    "    \n",
    "    for m in matches:\n",
    "        out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "with open('genetag/genetag.out', 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined genetag_long and genetag_short, i.e. genetag.out: ```0.592672838290329 Recall: 0.495634985999012```\n",
    "F1 = 0.540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype\n",
    "text = 'Peroxydase reaction stains were negative, chloroacetate esterase were strongly positive.'\n",
    "file_name = 'protein_name/protein_names_long'\n",
    "\n",
    "\n",
    "def get_proteins(s, f):\n",
    "    from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "    import os.path\n",
    "    from docria.printout import options\n",
    "    from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "    set_large_screen()\n",
    "    def connect_jvm(port):\n",
    "        from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "        gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "        gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "        app = gateway.entry_point\n",
    "        return gateway, gateway.jvm, app\n",
    "    gateway, jvm, app = connect_jvm(6006)\n",
    "    def get_java_file(path):\n",
    "        # Here we do the py4j equivalent for new java.io.File(path)\n",
    "        return jvm.java.io.File(os.path.abspath(path))\n",
    "    app.buildIndex(get_java_file(\"{}.txt\".format(f)), get_java_file(\"{}.fst\".format(f)))\n",
    "    indx = app.loadIndex(get_java_file(\"{}.fst\".format(f)))\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", s)\n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)\n",
    "    tuls = []\n",
    "    for term in doc[\"matches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "        #print(str(term[\"text\"]), \"-\", \"%d:%d\" % (term[\"text\"].start, term[\"text\"].stop))\n",
    "    print(tuls)\n",
    "    return dominant_right(tuls)\n",
    "\n",
    "matches = get_proteins(text, file_name)\n",
    "print('--------')\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do everything for both short and long names\n",
    "name = 'protein_name/protein_names_all';\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='uniprot_sprot.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "with open('{}.txt'.format(name), 'w') as f:\n",
    "    for event, element in parser:\n",
    "        tag = element.tag[length:]\n",
    "        if 'shortName' == tag or 'fullName' == tag:\n",
    "            proteins.add(element.text)  \n",
    "        element.clear()\n",
    "    f.write('\\n'.join(proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))\n",
    "############################################################################\n",
    "\n",
    "app.buildIndex(get_java_file('{}.txt'.format(name)), get_java_file('{}.fst'.format(name)))\n",
    "indx = app.loadIndex(get_java_file('{}.fst'.format(name)))\n",
    "\n",
    "\n",
    "#READ INPUT\n",
    "data = ''\n",
    "proteins = set()\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    #If it is an excerpt\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID'):\n",
    "                id = line.split(': ')[1]\n",
    "            elif line.startswith('TEXT'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        dictionary[id] = text\n",
    "    elif lines[0] == 'ANNOTATION':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('TEXT:'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        proteins.add(text)\n",
    "\n",
    "with open('{}.txt'.format(name), 'a', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(proteins))\n",
    "file.close()\n",
    "\n",
    "app.buildIndex(get_java_file('{}.txt'.format(name)), get_java_file('{}.fst'.format(name)))\n",
    "indx = app.loadIndex(get_java_file('{}.fst'.format(name)))\n",
    "############################################################################\n",
    "#import re\n",
    "d = dictionary\n",
    "out = []\n",
    "\n",
    "for key in d:\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", d[key])    \n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)  \n",
    "\n",
    "    tuls = []\n",
    "    for term in doc[\"matches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    matches = [(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "    \n",
    "    for m in matches:\n",
    "        #if re.search('(?!^\\d+$)^.+$', m[2]): #removes pure numbers\n",
    "            out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "with open('genetag/genetag_all.out'.format(p_type), 'w+', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method above did not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 9, 9/5-2019\n",
    "From last time we still had the following tasks in our todo list left:\n",
    "    3. Evaluate and find possible improvements\n",
    "    4. Add list of cell death synonyms\n",
    "    5. Fix demo that we can show (!)\n",
    "    \n",
    "However, last time we ran into a problem with the evaluator (again!). It seems that the Gold.format does not include all the bits that is included in the annotation bits. Some of those seem to be included in Correct.format. What is the difference in these two? Where is the bug?\n",
    "\n",
    "Only do the following snippets once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goes through the file uniprot_sprot.xml and saves the names of the proteins\n",
    "#in protein_names.txt\n",
    "name = 'short'\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='uniprot_sprot.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "for event, element in parser:\n",
    "    tag = element.tag[length:]\n",
    "    if 'shortName' in tag: # or 'shortName' in tag #fullName\n",
    "        proteins.add(element.text)\n",
    "    element.clear()\n",
    "with open('protein_names_{}.txt'.format(name), 'w+') as f:\n",
    "    f.write('\\n'.join(proteins))\n",
    "\n",
    "app.buildIndex(get_java_file(\"protein_name/protein_names_{}.txt\".format(name)), get_java_file(\"protein_name/protein_names_{}.fst\".format(name)))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name/protein_names_{}.fst\".format(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goes through the file uniprot_sprot.xml and saves the names of the proteins\n",
    "#in protein_names.txt\n",
    "name = 'long'\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "filename='uniprot_sprot.xml'\n",
    "parser = ET.iterparse(filename)\n",
    "length = 28 #length of header in xml tags\n",
    "proteins = set()\n",
    "for event, element in parser:\n",
    "    tag = element.tag[length:]\n",
    "    if 'fullName' == tag: # or 'shortName' in tag #fullName\n",
    "        proteins.add(element.text)\n",
    "    element.clear()\n",
    "f = open('protein_names_{}.txt'.format(name), 'w+')\n",
    "f.write('\\n'.join(proteins))\n",
    "f.close()\n",
    "\n",
    "#READ INPUT\n",
    "data = ''\n",
    "proteins = set()\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    if lines[0] == 'ANNOTATION':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('TEXT:'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        proteins.add(text)\n",
    "\n",
    "with open('protein_name/protein_names_{}.txt'.format(name), 'a', encoding='utf-8', errors='ignore') as file:\n",
    "    file.write('\\n'.join(proteins))\n",
    "\n",
    "app.buildIndex(get_java_file(\"protein_name/protein_names_{}.txt\".format(name)), get_java_file(\"protein_name/protein_names_{}.fst\".format(name)))\n",
    "indx = app.loadIndex(get_java_file(\"protein_name/protein_names_{}.fst\".format(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ ABSTRACTS TO FILE\n",
    "import json\n",
    "\n",
    "data = ''\n",
    "\n",
    "with open('genetag.db', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "sp = data.split('\\n>>')\n",
    "dictionary = {}\n",
    "\n",
    "for s in sp:\n",
    "    lines = s.split('\\n')\n",
    "    if lines[0] == 'EXCERPT':\n",
    "        id = ''\n",
    "        text = ''\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('EXCERPT_ID'):\n",
    "                id = line.split(': ')[1]\n",
    "            elif line.startswith('TEXT'):\n",
    "                text = ':'.join(line.split(':')[1:])[1:]\n",
    "        dictionary[id] = text\n",
    "\n",
    "with open('genetag/abstracts_genetag.txt', 'w+',encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file.write(json.dumps(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_type = 'long'\n",
    "\n",
    "from docria import Document, MsgpackCodec, DocumentIO, set_large_screen, T\n",
    "import os.path\n",
    "from docria.algorithm import group_by_span, dominant_right_span, dominant_right\n",
    "from docria.printout import options\n",
    "set_large_screen()\n",
    "def connect_jvm(port):\n",
    "    from py4j.java_gateway import GatewayParameters, JavaGateway\n",
    "    gateway_parameters=GatewayParameters(port=port, auto_convert=True, auto_field=True)\n",
    "    gateway = JavaGateway(gateway_parameters=gateway_parameters)\n",
    "    app = gateway.entry_point\n",
    "    return gateway, gateway.jvm, app\n",
    "gateway, jvm, app = connect_jvm(6006)\n",
    "def get_java_file(path):\n",
    "    return jvm.java.io.File(os.path.abspath(path))\n",
    "\n",
    "d = {}\n",
    "\n",
    "with open('genetag/abstracts_genetag.txt', 'r',encoding=\"utf-8\", errors='ignore') as file:\n",
    "    d = json.loads(file.read())\n",
    "\n",
    "    \n",
    "# Write to genetag\n",
    "out = []\n",
    "\n",
    "for key in d:\n",
    "    doc = Document()\n",
    "    doc.add_text(\"main\", d[key])    \n",
    "    binary_doc = MsgpackCodec.encode(doc) \n",
    "    search_binary_doc = app.search(indx, binary_doc)\n",
    "    doc = MsgpackCodec.decode(search_binary_doc)  \n",
    "\n",
    "    tuls = []\n",
    "    for term in doc[\"matches\"]:\n",
    "        tuls.append((term[\"text\"].start, term[\"text\"].stop, str(term[\"text\"])))\n",
    "    print(tuls)    \n",
    "    matches = [(x,y,z) for (x,y,z) in tuls if z in dominant_right(tuls)]\n",
    "    \n",
    "    for m in matches:\n",
    "        #if re.search('(?!^\\d+$)^.+$', m[2]): #removes pure numbers\n",
    "            out.append('|'.join([key, str(m[0]) + ' ' + str(m[1]), m[2]]))\n",
    "        \n",
    "file = open('genetag/genetag_{}.out'.format(p_type), 'w+', encoding=\"utf-8\", errors='ignore')\n",
    "file.write('\\n'.join(out))\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
