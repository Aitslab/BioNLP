{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logbook Adam & Ola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thursday 30th March 2021**  (Adam)  \n",
    "\n",
    "**Aim** - understand the task and expressions in the instructions.  \n",
    "\n",
    "**Step** -  I’ve understood that “named entities” are real world objects that can be named. NER stands for Named-Entity Recognition which is to locate and classify named entities, basically to look at the sentence and categorise the words. NEL stands for Named-Entity Linking, which is to link words with information, such as linking keywords in a wikipedia article.\n",
    "I’ve made the assumption that NER includes classification, making it NERC - will have to check though.  \n",
    "\n",
    "**Next step** - Check my Anaconda installation\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuesday 1st April 2021**  (Adam)  \n",
    "\n",
    "**Aim** - Check my Anaconda installation  \n",
    "\n",
    "**Step** -  I’ve had to reinstall my anaconda installation and I do believe it’s now working\n",
    "\n",
    "**Next step** - Download and run nlp_2021_alexander_petter\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monday 6th April 2021**  \n",
    "\n",
    "**Aim** - Download and run nlp_2021_alexander_petter  \n",
    "\n",
    "**Step** -  We've had some major problems with getting the code to run. We had to do some separate installations with pytorch to get it working. We removed the pytorch line in config, and installed it by itself through pytorch.org\n",
    "\n",
    "**Next step** - Continue\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuesday 7th April 2021**  \n",
    "\n",
    "**Aim** - Download and run nlp_2021_alexander_petter  \n",
    "\n",
    "**Step** -  Still struggling with getting the code to run, we have swapped methods by going to the downloader method instead of the cord_loader. we had problems with CUDA and had to use this guide: https://www.joe0.com/2019/10/19/how-resolve-tensorflow-2-0-error-could-not-load-dynamic-library-cudart64_100-dll-dlerror-cudart64_100-dll-not-found/ to make it work. In the end we got the downloader to work instead\n",
    "\n",
    "**Next step** - Continue\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thursday 8th April 2021**\n",
    "\n",
    "**Aim** - Download and run nlp_2021_alexander_petter  \n",
    "\n",
    "**Step** - We got the downloader to work, but couldn’t understand the next step.\n",
    "\n",
    "**Next step** - Take a break, ask and do reading while waiting\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sunday 11th April 2021**\n",
    "\n",
    "**Aim** - Read about BioBert  \n",
    "\n",
    "**Step** - BioBERT is a domain-specific language representation model pre-trained on large-scale biomedical corpora, which outperforms BERT in biomedical usace. We have read the paper and tried to understand the majority of its content.\n",
    "\n",
    "**Next step** - Run nlp_2021_alexander_petter\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monday 12th April 2021**\n",
    "\n",
    "**Aim** - Setup git  \n",
    "\n",
    "**Step** - Setup git for logbook etc. Just admi\n",
    "\n",
    "**Next step** - Setup Git, read instructions\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thursday 15th April 2021**\n",
    "\n",
    "**Aim** - Read about HunFlair  \n",
    "\n",
    "**Step** - HunFlair is a mix between a Flair character-level language model and fastText word embeddings trained on roughly 24 \n",
    "million abstracts from biomedical articles along with 3 million full text articles. We have read the paper and tried to \n",
    "understand its content. \n",
    "\n",
    "**Next step** - Setup Git repository, run the FlairNLP\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Friday 16th April 2021**\n",
    "\n",
    "**Aim** - Setup Git repository  \n",
    "\n",
    "**Step** - We created a new repository for our project. \n",
    "\n",
    "**Next step** - run the FlairNLP\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Friday 16th April 2021**\n",
    "\n",
    "**Aim** - run the FlairNLP  \n",
    "\n",
    "**Step** - We ran an predition example included in FlairNLP. It works fine.  \n",
    "\n",
    "**Next step** - Run the flair models on the output from downloader in nlp_2021_alexander_petter \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sunday 11th April 2021**\n",
    "\n",
    "**Aim** - Run nlp_2021_alexander_petter \n",
    "\n",
    "**Step** - We edited the config file and were able to run the steps downloader, sentencer, NER, analysis and metrics. It took approximately 4 hours to compute it all\n",
    "\n",
    "**Next step** - Setup Git, read instructions\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monday 19th April 2021**\n",
    "\n",
    "**Aim** - Start task \"train BioBERT NER models on Hunflair corpora\"  \n",
    "\n",
    "**Step** - We started the task of training BioBert ner models on Hunflair corpora. With the help of Marcus we found the Hunflair\n",
    "copora files. We are however stuck on the part of using these files to train BioBERT, the files are in a different format and we\n",
    "are confused as to how we should use them for training. \n",
    "\n",
    "**Next step** - Continue the task and and incorporate the Hunflair into nlp_2021_alexander_petter \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuesday 20th April 2021**\n",
    "\n",
    "**Aim** - Incorporate Hunflair corpora into nlp_2021_alexander_petter  \n",
    "\n",
    "**Step** - We spent a lot of time on trying to figure out how to use the hunflair corpora as input for nlp_2021_alexander_petter, the input formats did not match. Through discussion with Sonja and Marcus we decided to try to use dmis-lab/biobert (https://github.com/dmis-lab/biobert) to fine-tune bioBERT ourselves. The input used in this repository (IOB) is quite similar to that of hunflair. We will need to alter the hunflair files a bit to make it possible to train on. Today we started by downloading and installing the dmis-lab/biobert repository.   \n",
    "\n",
    "**Next step** - Write a script altering the Hunflair corpora to fit to biobert fine-tuning. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wednesday 21th April 2021**\n",
    "\n",
    "**Aim** - Write a script altering the Hunflair corpora to fit to biobert fine-tuning.  \n",
    "\n",
    "**Step** - We started writing a script to convert the HunFlair corpora files to the same format as the BioBert input. We also started trying to run the biobert dmis-lab/biobert (https://github.com/dmis-lab/biobert), but have run in to some problems. We changed for running it on our local machine to colab (GPU Hardware Accelerator) instead, to get it running. We are still having problems with the filetypes of BioBERT BioBERT-Large v1.1 (+ PubMed 1M) since they are named differently than the instructions states.\n",
    "\n",
    "**Next step** - Continue the script, and get our bioBERT to run\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monday 26th April 2021**\n",
    "\n",
    "**Aim** - Continue the script to convert Hunflair copora to fit bioBert fine-tuning  \n",
    "\n",
    "**Step** - The python script was completed and uploaded to BioNLP repo in github. The script works for an example of the\n",
    "hunflair corpora, to be able to use the script for fine-tuning the input and outputs will need to be specified and some other \n",
    "small changes might need to be done. But in general the script should work.\n",
    "\n",
    "**Next step** - Get bioBert finetuning to work\n",
    "***"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**Monday 26th April 2021**\n",
    "\n",
    "**Aim** - BioBert fine-tuning on bioBert corpora  \n",
    "\n",
    "**Step** - We solved the problems with file names, we renamed \"bio_bert_large_1000k.ckpt.data-00000-of-00001\" to \"model.ckpt-1000000\" together with naming the index and meta file to \"model.ckpt-1000000.index\", \"model.ckpt-1000000.meta\" and the vocabulary to \"vocab.txt\" and the config file to \"bert_config.json\"\n",
    "\n",
    "We then ran the command \n",
    "\n",
    "!python biobert/run_ner.py --do_train=true --do_eval=true --vocab_file=$BIOBERT_DIR/vocab.txt --bert_config_file=$BIOBERT_DIR/bert_config_bio_58k_large.json --init_checkpoint=$BIOBERT_DIR/model.ckpt-1000000 --num_train_epochs=10.0 --data_dir=$NER_DIR --output_dir=$OUTPUT_DIR\n",
    "\n",
    "However we encountered Out of memory error and had to modify the run_ner.py file from a batchsize of 32 to 12 and sequence length from 128 to 64. This helped but after running in collab for more than 1h the memory ran out again. The next step is to try a batch size of 6 and sequence length of 128. Hopefully this will solve the memory problem. \n",
    "\n",
    "**Next step** - Another try at fine-tuning BioBert\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuesday 27th April 2021**\n",
    "\n",
    "**Aim** - Another try at fine-tuning BioBert  \n",
    "\n",
    "**Step** - To solve the memory problem while fine-tuning we tried the process in google colab on Olas student account instead (last time we tried on Adams) and using biobert_base instead of biobert_large. The fine-tuning worked this way, we used a batch size of 32 and sequence size of 128. It was done on the biobert corpora JNLPBA. The next step is to do a sanity check by finetuning on the same corpora from Hunflair converted to fit the input.  \n",
    "\n",
    "**Next step** - Fine-tune biobert on converted JNLPBA from Hunflair as sanity check  \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thursday 29th April 2021**\n",
    "\n",
    "**Aim** - Fine-tune biobert on converted JNLPBA from Hunflair as sanity check   \n",
    "\n",
    "**Step** - We had the weekly meeting with Sonja and the other groups. We counted the B-tags of JNLPBA from hunflair compared to \n",
    "biobert and biobert has around 16000 more than Hunflair. \n",
    "We ran into a problem while trying to fine-tune on Hunflair, our converted corpora lacked newlines after each sentence which biobert has. We changed the conversion script and uploaded it to github. The training is now in progress. \n",
    "\n",
    "**Next step** - Evaluate the trained models   \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuesday 4th May 2021**\n",
    "\n",
    "**Aim** - Evaluate the trained modelsNext step\n",
    "\n",
    "**Step** - We evaluated three different models trained on biobert and three on hunflair. All models were evaluated on the dev and \n",
    "train corpora separately. So in total 12 different txt files were produced with the result shown by f-score, precision, recall \n",
    "and loss.   \n",
    "\n",
    "**Next step**  - Make tables of all results on github\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wednesday 5th May 2021**\n",
    "\n",
    "**Aim** - Make tables of all results on github\n",
    "\n",
    "**Step** - We made tables in the report and graph that we uploaded to github. The best model is the one trained with the most \n",
    "steps. For training the full hunflair corpora we will use 12 epochs instead of 10 to get even better performance. \n",
    "\n",
    "**Next step**  - Convert the full hunflair corpora for each category (gene/protein, decease and chemical) to then train on. We \n",
    "also need to convert the model files to ONNX format. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thursday 6th May 2021**\n",
    "\n",
    "**Aim** - Convert the full hunflair corpora for each category (gene/protein, decease and chemical)\n",
    "\n",
    "**Step** - We ran into some problems while converting the corpora for chemical. Some special characters · and ° gives us problems, since they are not recognized with UTF-8 encoding. We are trying to solve the problem. One way is to remove the sentences with these characters. Another could be to change · to * and ° for degrees.\n",
    "\n",
    "**Next step**  - Solve the conversion problem \n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
