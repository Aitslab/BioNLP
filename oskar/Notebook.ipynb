{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook - Oskar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Depdencies\n",
    "OS: Google Colab\n",
    "See installation specification in Readme.MD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Reading\n",
    "https://www.overleaf.com/read/xfdmhbzwshty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/3/26\n",
    "\n",
    "**Goal**: Setup and meet with supervisors.\n",
    "\n",
    "- First meeting with project leader. \n",
    "- Introduction to NPL in the biomedical field.\n",
    "- Created Repository & Setup Conda. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/3/30\n",
    "\n",
    "**Goal:** Explore coreference resolution.\n",
    "\n",
    "Read up on coreference resolution:\n",
    "- https://nlp.stanford.edu/projects/coref.shtml \n",
    "- https://towardsdatascience.com/intro-to-coreference-resolution-in-nlp-19788a75adee \n",
    "- https://towardsdatascience.com/most-popular-coreference-resolution-frameworks-574ba8a8cc2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/3/31\n",
    "\n",
    "**Goal:** Explore the card given by the supervison.\n",
    "\n",
    "Checked out tools and solutions\\\n",
    "https://github.com/huggingface/neuralcoref \\\n",
    "https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30\n",
    "\n",
    "Huggingface's neural coreference resolution tool is built using spaCy 2.1+ and \"annotates and resolves coreference clusters using neural networks\"\n",
    "\n",
    "Found one solution using nerualcoref with biomedical text: https://pypi.org/project/saber/ (Python).\n",
    "\n",
    "Checked out BERT and spanBERT\\\n",
    "https://github.com/mandarjoshi90/coref\n",
    "\n",
    "BERT stands for Bidirectional Encoder Representations from Transformers and uses a transformer-based machine learning technique for natural language processing. The coref library is the code from a paper exploring the use of BERT for coreference resolutions. \n",
    "https://arxiv.org/abs/1908.09091\n",
    "\n",
    "Another solution for coreference resolution in biomedical text: https://github.com/kilicogluh/Bio-SCoRes (Java).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/4/6\n",
    "**Goal:** Create a document for the report.\n",
    "\n",
    "Meeting with Supervisor \\\n",
    "Created document for the paper:\n",
    "https://www.overleaf.com/read/xfdmhbzwshty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/4/7\n",
    "\n",
    "**Goal:** Download the CRAFT corpus.\n",
    "\n",
    "Downloaded CRAFT from https://github.com/UCDenver-ccp/CRAFT.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/4/8\n",
    "\n",
    "**Goal**: Explore the CRAFT corpus.\n",
    "\n",
    "Looked at the format of the annotation in the CRAFT corpus. \\\n",
    "Looked up knowntator & read the wiki from the CRAFT github. \\\n",
    "Read the full article about identity chains, base noun phrase, apossitive relation and nonreferential pronoun. \\\n",
    "https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1775-9 \\\n",
    "https://github.com/UCDenver-ccp/CRAFT/wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/4/12\n",
    "\n",
    "**Goal:** Find tools that can be used for coreference resolution, preferably in python.\n",
    "\n",
    "Looked for tools regarding coref resolution.\n",
    "\n",
    "Candidates:\n",
    "- SABER: Sequence Annotator for Biomedical Entities and Relations. Which includes coreference resolution.\n",
    "https://baderlab.github.io/saber/ \n",
    "- BERT https://github.com/mandarjoshi90/coref \n",
    "- NeuralCoref https://github.com/huggingface/neuralcoref \n",
    "\n",
    "\n",
    "Looked at: https://sites.google.com/view/craft-shared-task-2019/results?authuser=0 to see the teams parciptating in the CRAFT 2019 competions but it does not seem like any of them are publically available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/4/13\n",
    "\n",
    "**Goal:** Descided on and download a coreference tool.\n",
    "\n",
    "Meeting with supervisor.\\\n",
    "Descided that i should explore saber, and if saber is not what were looking for i should go with BERT. Saber is from 2019 which migth be a bit outdated. Also there is no changes to the project for two years which may indicate that the project is abandoned. The same might be said for BERT or rather the project for coref with BERT which is also 2 years old.\n",
    "\n",
    "Installing saber with: pip install git+https://github.com/BaderLab/saber.git which is taking some time. Could not download it with pip due to keras not being avaiable as a pip download. The download is taking a while...\n",
    "\n",
    "**TODO:** Install and try out Saber."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/4/14 \n",
    "\n",
    "**Goal:** Installing saber\n",
    "\n",
    "Downloading and installing saber took longer than expected, there seems to be some broken dependency and im trying out a couple of fixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/4/15 - 2021/4/16\n",
    "\n",
    "**Goal:** Installing saber\n",
    "\n",
    "Build is failing and dependencies are broken so therefor i try the SpanBERT Coref resolution package instead. There seems to be some failing depdencies on the spanBERT aswell but at least the build is not failing.\n",
    "\n",
    "**TODO**: Fix the depdencies issues on spanBERT and hopefully install it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/4/19\n",
    "**Goal**: Installing SpanBERT\n",
    "\n",
    "Seems like tensorflow-gpu does not exists for Mac will try to install it on my Windows PC if there is no workaround.\n",
    "\n",
    "Installed SpanBERT on my Windows PC with python version 3.5.\n",
    "\n",
    "**TODO**: Try out SpanBERT and read up on how to train new models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/4/20 - 2021/4/21\n",
    "\n",
    "**Goal**: Try out SpanBERT\n",
    "\n",
    "This task was harder than expected cause of the tool not being a library like saber and nerualcoref. Therefor i have to use the file downloaded from the github and try to work out how to use the tool. There is also no apparent instructions on how to run it so i have to experiment on how to use it. Ive also run into a problem with nvcuda.dll cause i dont have a nivida graphics card. Im trying to find a workaround to this problem otherwise il have to use a cluster or Google Colab.\n",
    "\n",
    "**TODO:** Run a prediction with SpanBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/4/22\n",
    "\n",
    "**Goal**: Make a prediction with SpanBERT\n",
    "\n",
    "Found a good Google Colab notebook which already setup SpanBERT for use, tried this with the a random text and the pretrained model and got a prediction. Next step would be to find out how you train new models. \n",
    "\n",
    "**TODO:** Figure out how to train models with SpanBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/4/22\n",
    "\n",
    "**Goal:** Supervisor meeting \n",
    "\n",
    "Met with the supervisor and descided on the work to do glancing forward. \n",
    "\n",
    "**TODO:** \n",
    "- Check or convert CRAFT so we can use it with SpanBERT\n",
    "- Evaluate SpanBERT base model on CRAFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/4/26\n",
    "\n",
    "**Goal:** Look at resources provided by supervisor.\n",
    "\n",
    "Looked at the Google Colab provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/4/27\n",
    "\n",
    "**Goal:** Setup Google Colab with SpanBERT\n",
    "\n",
    "Made a own google colab with SpanBERT.\n",
    "\n",
    "**TODO:** Put up download instructions, write about saber in report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/4/28\n",
    "**Goal**: Installation instructions.\n",
    "\n",
    "Updated README.md with installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/4/28\n",
    "**Goal**: Supervisor meeting\n",
    "\n",
    "Got a transformed dataset from nicholas. Next step is to try to apply the coref tool to this set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/3\n",
    "**Goal**: Run SpanBERT with CRAFT.\n",
    "\n",
    "Started working on running SpanBERT with the CRAFT corpus to produce a baseline. Uploading the craft files to google colab but im quite uncertain on what is the most comfortable action. Decided on uploading it to drive then mounting my drive unto the colab which seems to be working. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/4\n",
    "\n",
    "**Goal**: Run a prediction on the CRAFT corpus.\n",
    "\n",
    "Tried to run predicitons on the CRAFT corpus, unfortunately something with the parser is broken, cause the conversion from txt to JSON Lines seems to be broken. Tried to look at the parser and try to understand how it works and also been looking at the documentation of JSONL. Will ask this on the 5/5 meeting. Also uncertain on which genre for the Ontonotes i should be using. Also the notebook does not seem to support text with multiple speakers at the moment which probably would need to be changed.\n",
    "\n",
    "**TODO**: Write a parser from the CRAFT text files to JSON Lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/5\n",
    "**Goal:** Run a prediction on the CRAFT corpus.\n",
    "\n",
    "Managed to find one error with the parser and solved it by removing empty lines when importing data from the corpus file. Managed to get it to run a prediction on a couple of sentences but when im trying more one of them gets an assertion error. The assertion error has something do with length of a input mask and sum of words but cant find the connection yet.\n",
    "\n",
    "Also met with supervisor and the next steps are quite clear:\n",
    "\n",
    "- Prediction on whole .txt article\n",
    "- Prediction on whole corpus.\n",
    "- Try to evaulate the result to the annontated corpus. \n",
    "- Train a new model and run prediction.\n",
    "\n",
    "Ran a nltk tokenzier on the text instead and got it working for [0:100] sentences. Running it on the whole txt files seems to crash cause of something with memory allocation which im looking into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/6\n",
    "\n",
    "**Goal**: Find out whats causing the memory problem.\n",
    "\n",
    "Seems that the issue for not being able to run the whole txt file is a VRAM issue. Currenlty ive only gotten 16GB VRAM which does not seem enough, tried to change ffnn_size but probably need the original ontonotes to be able to change it cause of checkpoints. If the problem persits i might have to move on to working with a cluster. Also found some small instructions on how to train with a new corpus and im eager to test it when the baseline is solved.\n",
    "\n",
    "Im aslo awaiting txt. files from nicholas so that we are running on exactly the same corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/10\n",
    "\n",
    "**Goal**: Find out whats causing the memory problem.\n",
    "\n",
    "To single out if the system just lacks VRAM i tried to run the bert model instead of the span bert, unfortunately google colab have not given me a single GPU over the weekend and today which is a shame. Can also try to finetune the spanbert model using a smaller ffnn size but as with the prediction it runs out of memory while training as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/11\n",
    "\n",
    "**Goal:** Start writing on the paper.\n",
    "\n",
    "Multitasking with an another project today but got a little done on the report. Started writing about CRAFT and Joshi's coref tool also introduced some references.\n",
    "\n",
    "Report TODO:\n",
    "- Introcution (Coref, Biomedical data, covid-19, craft, Joshi, Saber)\n",
    "- Method (Craft - preprocess & format, Joshi - tokenization, baseline / pretrained, training with craft) \n",
    "- Result\n",
    "- Conclusion\n",
    "- Abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/12\n",
    "\n",
    "**Goal**: Supervisor meeting\n",
    "\n",
    "Discussed ongoing problems and how to proceed with the project. Talked about cluster access and splitting text into small predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/14\n",
    "\n",
    "**Goal**: Cluster access and batch prediction\n",
    "\n",
    "Got access and information about the clusters which i migth use if the training/prediction is not working on colab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/14 - 16\n",
    "\n",
    "**Goal:** Make prediction on small batches.\n",
    "\n",
    "Split the articles into smaller parts and ran prediction on these. This took longer than expected because the library does not have any functionality to run from an another python script. Had to setup subprocesses and enviorments and finally got it running predictions. These took around a quarter hour to make with 7 articles. The prediction data was split into around five parts per article which yielded around 35 files. \n",
    "\n",
    "The next step is to do evaulation which ive started on. The problem is that the way the prediction data is strucutred i just cant convert them into conll without heavy preprocessing which makes it very hard to evaulate. Im currently working on comparission between the two formats and ive found a library that can help which is called coref conversion. \n",
    "\n",
    "Unfortunatly on how the data is strucutured this is taking longer than i would like, if i dont make any more progresss on sunday i may ask for help on how to convert and compare the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/17\n",
    "\n",
    "**Goal**: Start training the model & Fix output data.\n",
    "\n",
    "Ran a prediction on the whole test set as well, which produced around 151 files. These files will also need to be parsed so they can be evaulated.\n",
    "\n",
    "While figuring out the data problem ive started working on training the model, trying to do the setup so i can get a prediction going with the trained model as well. Because the data from the trained model and the baseline model is the same il need to parse this data as well. Hopefully i solve the problem but untill then i try to get as much data as possible so i can just get evaulations after the conversion/parsing functions are done. \n",
    "\n",
    "The training is taking a while but i does not seem like i have a memory problem yet, running with a very small jsonlines file at the moment untill i get the minimize function to work for the CRAFT data. Have not found a batch size setting yet which i kind of worrying. The two setting i could find so far is max_segment_lenght and ffn_size. \n",
    "\n",
    "Managed to get some models with training, used wrong dev set and only used a small article for train set but i got it to work so now its just replacing these and running it again. \n",
    "\n",
    "The training takes a lot of time though, but it saves model with checkpoints and also saves the best model as a 'max model'. This could be useful because we migth not need to train it as long because of the size of our dataset. The train script also displays loss and step and f1/recall/presicion on every checkpoint. When testing the training the default epoch was set to 20 which is quite large, i will defenitly change this value next test.\n",
    "\n",
    "Right now i cant plug in a whole article for the training because the built in parser complains when the annontation uses letters seems like a easy fix but i need to lookup why the format varies between the tool/ontonotes and the corpus. The varying formats used in the tool becomes quite annoying and confusing and there is no specified format for conll in the project, they only specifies that its used.\n",
    "\n",
    "**TODO:** \n",
    "- Change train file to a bigger one (including all articles?)\n",
    "- Change dev file for proper evaulation on the training.\n",
    "- Make predictions with the trained model\n",
    "- Fix the output so we can make evaulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/18\n",
    "\n",
    "**Goal:** Use the cluster to make a prediction to be able to use evaulate function on whole articles.\n",
    "\n",
    "Started working with the cluster, got a folder setup copied the corpus and downloaded coref. Got stuck on requirments file because some of the requirments wont build but im trying to figure out a workaround.\n",
    "\n",
    "The errors: \n",
    "ERROR: Failed building wheel for h5py\n",
    "ERROR: Failed building wheel for PyYAML\n",
    "\n",
    "Probably has something with either python version or not being able to install dev tools on the cluster. Couldent use the same python version as the one i used on my local machine or the one being used on colab which makes it harder to track down the issue. \n",
    "\n",
    "Also started working on transforming the colab into python files so that i can run it easier on the cluster and downloaded the pretrained model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/19\n",
    "\n",
    "**Goal:** Use the cluster to make a prediction & Supervisor Meeting\n",
    "\n",
    "Got the cluster working and made a predicton on a full article which looks promising, now its just getting an agreement on the format between the tool and the corpus for the evaulation formula. Need to fix the doc_key and also fix how annonations are made to make them up to par with eachother. When the conll file works as an input for the eval function it should produce us a proper baseline which is great. Have not got to look at training in the cluster yet but i think the steps probably would be:\n",
    "\n",
    "Training:\n",
    "\n",
    "- Create a big conll file containg multiple articles for training because of how the train script is built.\n",
    "- Do the same for the dev set to be able provide information while training.\n",
    "- See if i can find batchsize and reduce it\n",
    "- Reduce epochs to something reasonable.\n",
    "- Try to train it on the cluster or colab preferably the cluster.\n",
    "- Do evaulation on the cluster to make it easier.\n",
    "\n",
    "\n",
    "The minimize function takes three conll files train/dev/test and reduces these to jsonlines for training. By getting the conll files to work with this function training would be a lot easier. Right now the minimize function complains on letters in the annontation.\n",
    "\n",
    "For baseline & evaulation:\n",
    "- Get conll file to work with the eval function\n",
    "- combine the results for each article.\n",
    "- If it does not work before the presentation. Manually compare and look if the result is accepatble. Probably a last hail mary cause i really want got get it working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/20 \n",
    "\n",
    "**Goal:** Evaluate predictions.\n",
    "\n",
    "Fixed the issue with doc_key and got an evaulation on a whole article going, which is great. The bad news is that the evaluation is not great. If its correct we have a f1 score of around 0.31% which is terrible. Manually going through it to see if the evaulation is correct and if it is we really need the training.\n",
    "\n",
    "To get the evaluation going on the cluster i made a new script with subprocesses. Have not yet tested it cause i need to find someway of saving the result from each round.\n",
    "\n",
    "\n",
    "Made some changes to the minimze.py script and saving it here if i forget:\n",
    "\n",
    "- (67) cluster_id = int(re.sub(\"\\D\", '', part[1:-1])) : cluster_id = int(part[1:-1])\n",
    "- (70) cluster_id = int(re.sub(\"\\D\", '', part[1:])) : cluster_id = int(part[1:])\n",
    "- (73) cluster_id = int(re.sub(\"\\D\", '', part[:-1])) : cluster_id = int(part[:-1])\n",
    "- (96) Removed assert len(all_mentions) == len(set(all_mentions)) migth not be good but have not found a solution so far.\n",
    "\n",
    "\n",
    "Combined all my conll files from the dev set to one big file, ran minimize on that file to produce the tokenized txt versions then ran a evaluation on the 7 documents.\n",
    "\n",
    "Which gave the following result:\n",
    "\n",
    "```\n",
    "Average F1 (conll): 19.40%\n",
    "Average F1 (py): 25.88% on 7 docs\n",
    "Average precision (py): 55.24%\n",
    "Average recall (py): 17.01%\n",
    "\n",
    "```\n",
    "\n",
    "And the values:\n",
    "\n",
    "```\n",
    "Official result for muc\n",
    "version: 8.01 /cephyr/NOBACKUP/groups/snic2021-23-312/Oskar/coref/conll-2012/scorer/v8.01/lib/CorScorer.pm\n",
    "\n",
    "====== TOTALS =======\n",
    "Identification of Mentions: Recall: (10399 / 35390) 29.38%\tPrecision: (10399 / 14258) 72.93%\tF1: 41.89%\n",
    "--------------------------------------------------------------------------\n",
    "Coreference: Recall: (7567 / 32240) 23.47%\tPrecision: (7567 / 13375) 56.57%\tF1: 33.17%\n",
    "--------------------------------------------------------------------------\n",
    "\n",
    "Official result for bcub\n",
    "version: 8.01 /cephyr/NOBACKUP/groups/snic2021-23-312/Oskar/coref/conll-2012/scorer/v8.01/lib/CorScorer.pm\n",
    "\n",
    "====== TOTALS =======\n",
    "Identification of Mentions: Recall: (10399 / 35390) 29.38%\tPrecision: (10399 / 14258) 72.93%\tF1: 41.89%\n",
    "--------------------------------------------------------------------------\n",
    "Coreference: Recall: (2835.09467478983 / 35390) 8.01%\tPrecision: (2756.79323299393 / 14258) 19.33%\tF1: 11.32%\n",
    "--------------------------------------------------------------------------\n",
    "\n",
    "Official result for ceafe\n",
    "version: 8.01 /cephyr/NOBACKUP/groups/snic2021-23-312/Oskar/coref/conll-2012/scorer/v8.01/lib/CorScorer.pm\n",
    "\n",
    "====== TOTALS =======\n",
    "Identification of Mentions: Recall: (10399 / 35390) 29.38%\tPrecision: (10399 / 14258) 72.93%\tF1: 41.89%\n",
    "--------------------------------------------------------------------------\n",
    "Coreference: Recall: (276.787649230821 / 3150) 8.78%\tPrecision: (276.787649230821 / 883) 31.34%\tF1: 13.72%\n",
    "--------------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "Im a little lost in all the values, but from what i can understand we have a better score for identifying mentions than coreferences (41% to 13%-33%).\n",
    "\n",
    "Next up is doing the same with the test and train set. Combining these will be harder because of the enormous amount of data. But if its completed i hope i can do the same evaluation on the test set and also train the model.\n",
    "\n",
    "I also seem to have a problem with not using GPU when running these jobs. It seems like they only use the CPU and system memory from monitoring the job on Alvis. Dont know if its a bug with the monitoring but if i run into problems with the bigger set this migth be why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/21\n",
    "\n",
    "**Goal:** Evaluate Predictions \n",
    "\n",
    "Merged both the test files and train files into two single CoNLL documents. These are pretty massive but it seems with the given time the tool is able to use them. Ran baseline predicitons/evaluations on the test set as well and got a similar score to the dev set which is reasonable because the tool has not been training on the dev set. The prediction + evaluation took around 2 hours. \n",
    "\n",
    "The result are the following:\n",
    "\n",
    "```\n",
    "Average F1 (conll): 14.32%\n",
    "Average F1 (py): 23.56% on 30 docs\n",
    "Average precision (py): 55.67%\n",
    "Average recall (py): 15.14%\n",
    "```\n",
    "\n",
    "Which seems quite abyssmal as well. Both these evaluations seems to be running on the CPU:s which migth hinder performance. Im trying to get it to work with the GPU's but no luck at the moment. Before attempting to train i need to sovle this GPU problem because Joshi mentions that the memory of the GPU greatly affects the performance of the models, and if im running the training with a CPU if that is even possible it migth hinder the results were looking for.\n",
    "\n",
    "I think i got it working with the GPU, so im gonna rerun it with bigger GPU:s, all are busy at the moment so i submitted the job and im waiting for it to run. If the result are drastically different on the dev set i migth run it on the test set as well otherwise i will begin training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/21\n",
    "\n",
    "**Goal**: Using GPU on predictions.\n",
    "\n",
    "Got it running with the bigger GPU but still ran into memory problems which is a shame. Migth try to split one CoNLL document into many parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/21\n",
    "\n",
    "**Goal**: Using GPU on predictions.\n",
    "\n",
    "Tried to split a CoNLL article into multiple parts but the minmize script provided with the tool wont work with it. It could be that it don't recognize parts or that the coref mentions does not work well with splitting. Instead of putting to much effort into solving this, i will start with training and then evaluate using CPU instead because it seems like that worked. \n",
    "\n",
    "Started working on training, it seems like it works fine with memory untill its gonna run its first evaluation on the model where it crashes from OOM. This means i probably have to find a way to reduce the size of the dev set documents or split them into parts which was harder than expected.\n",
    "\n",
    "Managed to split the CoNLL documents into smaller parts and ran a evaluation on the dev set on a GPU, this took longer than running on a CPU which is rather bizzare but i managed to get an result after around 1.5 hours. Because the result did not vary significantly i wont run the evaluation of test on gpu.\n",
    "\n",
    "With this working i probably gonna get the training to work, but with the 1.5 hours of eval its gonna be painfully slow. Probably starting the training tomorow morning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/24\n",
    "\n",
    "**Goal**: Training & Presentation\n",
    "\n",
    "Started working on training and started with the presentation. Met with the supervisor and discussed presentation and values etc. Had some problems with training only running one epoch then closing but il try to fix it tommorow cause ive got a lot of other course work to do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/25\n",
    "\n",
    "**Goal:** Training\n",
    "\n",
    "Got training to work and im currenlty running it. I think expected time is around 12 hours.\n",
    "\n",
    "Training finished and took around 13 hours, best model was around the tenth epoch.\n",
    "\n",
    "Starting evaluation on development set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021/5/26\n",
    "\n",
    "**Goal:** Evaluation of trained model\n",
    "\n",
    "The evaluation i started yesterday was completed and ive uploaded the result on github. \n",
    "\n",
    "Started evaluation on test set now.\n",
    "\n",
    "Evaluation on test set was also completed, there were defenitly improvement. Putting this on github as well.\n",
    "\n",
    "Worked on presenation pretty much the whole day, feeling quite finished with the slider, need some work on the script but hopefully i will have it finished today.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
