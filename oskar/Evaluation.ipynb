{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1vCbGi5NeMA"
      },
      "source": [
        "## Evalatuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0eYefSpNgCc",
        "outputId": "bd908c2a-9cef-44bb-f40f-4cf442f044fa"
      },
      "source": [
        "! git clone https://github.com/boberle/corefconversion.git"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'corefconversion'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 61 (delta 18), reused 57 (delta 14), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (61/61), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfLEBBlSNvOz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c87b5a51-0222-45ae-c565-4d50f2194510"
      },
      "source": [
        "!unzip out.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  out.zip\n",
            "   creating: content/coref/data/out/\n",
            "  inflating: content/coref/data/out/17696610_3.json  \n",
            "  inflating: content/coref/data/out/17244351_1.json  \n",
            "  inflating: content/coref/data/out/17590087_0.json  \n",
            "  inflating: content/coref/data/out/17590087_2.json  \n",
            "  inflating: content/coref/data/out/17608565_1.json  \n",
            "  inflating: content/coref/data/out/17425782_2.json  \n",
            "  inflating: content/coref/data/out/17244351_0.json  \n",
            "  inflating: content/coref/data/out/17194222_1.json  \n",
            "  inflating: content/coref/data/out/17447844_2.json  \n",
            "  inflating: content/coref/data/out/17425782_0.json  \n",
            "  inflating: content/coref/data/out/17194222_2.json  \n",
            "  inflating: content/coref/data/out/17425782_4.json  \n",
            "  inflating: content/coref/data/out/17696610_1.json  \n",
            "  inflating: content/coref/data/out/17608565_0.json  \n",
            "  inflating: content/coref/data/out/17590087_4.json  \n",
            "  inflating: content/coref/data/out/17194222_3.json  \n",
            "  inflating: content/coref/data/out/17244351_4.json  \n",
            "  inflating: content/coref/data/out/17696610_4.json  \n",
            "  inflating: content/coref/data/out/17608565_3.json  \n",
            "  inflating: content/coref/data/out/17590087_1.json  \n",
            "  inflating: content/coref/data/out/17447844_3.json  \n",
            "  inflating: content/coref/data/out/17696610_0.json  \n",
            "  inflating: content/coref/data/out/17608565_4.json  \n",
            "  inflating: content/coref/data/out/17425782_3.json  \n",
            "  inflating: content/coref/data/out/17590087_3.json  \n",
            "  inflating: content/coref/data/out/17244351_2.json  \n",
            "  inflating: content/coref/data/out/17447844_0.json  \n",
            "  inflating: content/coref/data/out/17447844_4.json  \n",
            "  inflating: content/coref/data/out/17608565_2.json  \n",
            "  inflating: content/coref/data/out/17425782_1.json  \n",
            "  inflating: content/coref/data/out/17244351_3.json  \n",
            "  inflating: content/coref/data/out/17447844_1.json  \n",
            "  inflating: content/coref/data/out/17194222_0.json  \n",
            "  inflating: content/coref/data/out/17696610_2.json  \n",
            "  inflating: content/coref/data/out/17194222_4.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82sbtMpwOILv",
        "outputId": "9c1f5f32-2f52-457a-b12c-875a7249ddfd"
      },
      "source": [
        "%cd corefconversion/"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'corefconversion/'\n",
            "/content/corefconversion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91U5fiq8OzeM",
        "outputId": "6912350b-3d46-47f1-c47b-1de3f2b3cff4"
      },
      "source": [
        "! python jsonlines2conll.py -h"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: jsonlines2conll [-h] [-g] [-t] [-T] -o OUTFPATH [-c CONLL_FILES]\n",
            "                       [--cols COLS]\n",
            "                       infpaths [infpaths ...]\n",
            "\n",
            "convert jsonlines to conll\n",
            "\n",
            "positional arguments:\n",
            "  infpaths              input files\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -g, --gold            use gold clusters instead of predicted clusters\n",
            "  -t, --in-tab-sep      input conll files use tab as separator\n",
            "  -T, --out-tab-sep     output conll files use tab as separator\n",
            "  -o OUTFPATH           output file\n",
            "  -c CONLL_FILES, --conll CONLL_FILES\n",
            "                        conll files to merge with, may be repeated\n",
            "  --cols COLS           comma separated list of cols to include, in order\n",
            "                        (default: 'sentences')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPwm0gdZOSS5"
      },
      "source": [
        "! python jsonlines2conll.py /content/out/17194222_0.json -o ouput.conll"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rApQ4oe0QrKz"
      },
      "source": [
        "import json\n",
        "output = json.load(open(\"/content/out/17194222_0.json\"))\n",
        "\n",
        "comb_text = [word for sentence in output['sentences'] for word in sentence]\n",
        "\n",
        "def convert_mention(mention):\n",
        "    start = output['subtoken_map'][mention[0]]\n",
        "    end = output['subtoken_map'][mention[1]] + 1\n",
        "    nmention = (start, end)\n",
        "    mtext = ''.join(' '.join(comb_text[mention[0]:mention[1]+1]).split(\" ##\"))\n",
        "    return (nmention, mtext)\n",
        "\n",
        "seen = set()\n",
        "print('Clusters:')\n",
        "for cluster in output['predicted_clusters']:\n",
        "    mapped = []\n",
        "    for mention in cluster:\n",
        "        seen.add(tuple(mention))\n",
        "        mapped.append(convert_mention(mention))\n",
        "    print(mapped, end=\",\\n\")\n",
        "\n",
        "print('\\nMentions:')\n",
        "for mention in output['top_spans']:\n",
        "    if tuple(mention) in seen:\n",
        "        continue\n",
        "    print(convert_mention(mention), end=\",\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5Y0WSudSMl4",
        "outputId": "5297dde4-f163-4522-df55-40209684deb4"
      },
      "source": [
        "import conll_transform\n",
        "\n",
        "def merge(x):\n",
        "\n",
        "  answer = []\n",
        "  current = \"\"\n",
        "\n",
        "  for item in x:\n",
        "    if not item.startswith(\"##\"):\n",
        "      if current:\n",
        "        answer.append(current)\n",
        "        current = item\n",
        "      else:\n",
        "        current = item\n",
        "    else:\n",
        "        current = current + item[2:]\n",
        "\n",
        "  answer.append(current)\n",
        "  return answer\n",
        "\n",
        "\n",
        "output = json.load(open(\"/content/out/17194222_0.json\"))\n",
        "comb_text = [word for sentence in output['sentences'] for word in sentence]\n",
        "\n",
        "#sentences = [[] for x in range(max(output['sentence_map']) + 1)]\n",
        "\n",
        "#for i, n in enumerate(output['sentence_map']):\n",
        "#  sentences[n].append(comb_text[i]) \n",
        "\n",
        "#chains = output['predicted_clusters']\n",
        "#mentions = [ m for chain in chains for m in chain ]\n",
        "\n",
        "#conll_transform.textpos2sentpos(mentions, sentences)\n",
        "\n",
        "#for sentence in sentences:\n",
        "#  sent.append(merge(sentence))\n",
        "\n",
        "\n",
        "docs = dict()\n",
        "\n",
        "data = output\n",
        "doc_key = data[\"doc_key\"]\n",
        "for sent in zip(*[iter(data['sentences'])]):\n",
        "  print(len(sent[0]))\n",
        "\n",
        "sents = [[list(token) for token in zip(*sent)] for sent in zip(*[iter(data['sentences'])])]\n",
        "#print(sents)\n",
        "\n",
        "chains = data['predicted_clusters']\n",
        "\n",
        "mentions = [ m for chain in chains for m in chain ]\n",
        "conll_transform.textpos2sentpos(mentions, sents)\n",
        "\n",
        "conll_transform.write_chains(sents, chains, append=True)\n",
        "\n",
        "docs[doc_key] = sents\n",
        "\n",
        "#print(docs)\n"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "358\n",
            "367\n",
            "375\n",
            "346\n",
            "358\n",
            "384\n",
            "330\n",
            "357\n",
            "369\n",
            "342\n",
            "309\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}